---
title: Feedforward Neural Networks
subtitle: FYS-STK4155 - Project 2
authors:
  - name: Insert Name
site:
  template: article-theme
exports:
  - format: pdf
    template: ../report_template
    output: report.pdf
    showtoc: true
math:
  # Note the 'single quotes'
  '\argmin': '\operatorname{argmin}'
  '\drm': '\mathrm{d}'
  '\R': '\mathbb{R}'
  '\Set': '{\left\{ #1 \right\}}'
bibliography: references.bib
abstract: |
  This project studies feedforward neural networks (FFNN) applied to regression and binary classification tasks. Initially, a custom FFNN regression model was trained on synthetic data generated by the Franke function. Subsequently, a custom FFNN binary classification model was trained using the [Wisconsin breast cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). Parameter tuning was performed via random grid search to assess the sensitivity of the models to the number of hidden layers and neurons. The performance of the models was evaluated with various activation functions, including sigmoid, rectified linear unit (ReLU), and leaky ReLU (LReLU). Overall, the ReLU-type activations consistently outperformed the sigmoid function in both regression and classification tasks. Additionally, the FFNN models were compared to traditional linear modelsâ€”ordinary least squares (OLS) and ridge regression for regression tasks, and logistic regression for classification. In both cases, the linear models demonstrated performance comparable to that of the FFNN models, but with significantly lower computational costs.

---

# Introduction

This paper explores the application of FFNNs in both regression and binary classification tasks, focusing on their performance compared to traditional linear models.

In the regression component of this study, we utilize synthetic data generated by the Franke function, a well-known benchmark in the field of regression analysis. The FFNN regression model is designed to assess how well these networks can approximate complex functions and identify the impact of various hyperparameters, including the number of hidden layers and neurons.

In the classification segment, we employ a FFNN model trained on the Wisconsin breast cancer dataset, a widely used dataset for binary classification tasks. Here, we examine the effectiveness of different activation functions, specifically sigmoid, rectified linear unit (ReLU), and leaky ReLU (LReLU), in improving model accuracy.

To ensure robust model performance, parameter tuning is conducted using a random grid search method. This systematic approach allows us to evaluate the sensitivity of FFNNs to their architectural choices and optimize their configurations.

Importantly, this study also contrasts the performance of FFNNs with that of linear models, including ordinary least squares (OLS), ridge regression, and logistic regression. Preliminary findings suggest that while FFNNs can yield competitive results, traditional linear models often provide similar predictive power at a fraction of the computational cost. By examining these models side by side, this paper aims to provide insights into the trade-offs involved in selecting machine learning techniques for various predictive tasks.

# Theory and Method

## Gradient Descent

The core problem in machine learning is to parameter estimation by minimizing a scalar cost function $C:\Theta\to\R$, where $\Theta$ denotes the parameter space. This optimization problem can be stated as

$$
  \hat{\boldsymbol{\theta}} \in \argmin_{\boldsymbol{\theta}\in\Theta} C(\boldsymbol{\theta})
$$

Gradient descent is a first-order optimization algorithm that iteratively updates the parameters in the direction of negative gradient of the cost function. Specifically, the update rule takes the form

$$
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t \nabla_{\boldsymbol{\theta}_t} C(\boldsymbol{\theta})
$$

where $\eta_t$ is the *learning rate*. This method relies on the principle from multivariable calculus that the gradient points in the direction of the steepest ascent, which means that moving in the opposite direction (the negative gradient) will lead to a decrease in the cost function. 

### Gradient Descent for Linear Regression

The cost function for an ordinary least squares (OLS) regression problem has the form

$$
  C(\boldsymbol{\beta}) = \frac{1}{2N} \sum_{n=1}^N (\mathbf{x}_i^\top \boldsymbol{\beta} - y_n)^2 = \frac{1}{2N}\lVert\mathbf{X}\boldsymbol{\beta} - \mathbf{y}\rVert_2^2,
$$

where, $\mathbf{X}$ is the design matrix, $\boldsymbol{\beta}$ are the regression coefficients, and $\mathbf{y}$ are the observed values. The gradient of $C$ is given by

$$
\label{equation-ols-gradient}
  \mathbf{g}_t = \frac{1}{N} \sum_{n=1}^N (\boldsymbol{\beta}_t^\top \mathbf{x}_n - y_n)\mathbf{x}_n = \frac{1}{N}\mathbf{X}^\top (\mathbf{X}\boldsymbol{\beta} - \mathbf{y}) 
$$

and Hessian matrix is expressed as:

$$
  \mathbf{H}(\mathbf{X}) = \frac{2}{N}\mathbf{X}^\top \mathbf{X}
$$

Since the matrix product $\mathbf{X}^\top \mathbf{X}$ is always positive semi-definite, it follows that $C$ is a convex function. The convexity guarantees that every local minimum of $C$ is also a global minimum. Using the gradient [](#equation-ols-gradient), we can derive a closed form the gradient descent update expressed as:

$$
  \boldsymbol{\beta}_{t + 1} = \boldsymbol{\beta}_t - \eta_t \mathbf{g}_t = \eta_t (\boldsymbol{\beta}_t^\top - \mathbf{x}_n - y_n) \mathbf{x}_n.
$$

### Learning Rates

There are several ways of managing the learning rates, or step sizes, in gradient descent optimization. The simplest method is to use a constant learning rate $\eta_t = \eta$ in all update steps $t$. However, this method may fail to converge if $\eta$ is too large, and if it is too small, the method will converge but very slowly.

#### Momentum Method

Another method of gradient descent is to scale gradient in each update step via the update

$$
\begin{align*}
  \mathbf{m}_t =& \gamma\mathbf{m}_{t-1} + \mathbf{g}_{t-1} \\
  \boldsymbol{\theta}_t =& \boldsymbol{\theta}_{t-1} - \eta_t \mathbf{m}_t
\end{align*}
$$

where $\mathbf{m}_t$ is the momentum and $\gamma\in(0,1)$. A typical value of $\gamma$ is $0.9$ [@book_murphy_2022]. When $\gamma = 0$, the method reduces to gradient descent. Expandind the momentum update recursively, we see that $\mathbf{m}_t$ behaves like an exponentially weighted moving average of the past gradients, i.e.

$$
  \mathbf{m}_t = \sum_{\tau=0}^{t-1} \gamma^\tau \mathbf{g}_{t-\tau - 1}
$$

If all past gradients are constant, this simplifies to $\mathbf{m}_t = \mathbf{g} \sum_{\tau=0}^{t-1} \gamma^\tau$, in which case the scaling factor is a geometric series:

$$
  \sum_{\tau=0}^\infty \gamma^\tau = \frac{1}{1 - \gamma}
$$

Thus in the limit $\tau\to\infty$, we multiply the gradient by $1/(1 - \gamma)$.

#### Learning Rate Scheduler

Rather than choosing a single constant learning rate, we can use a learning rate schedule, adjusting the step size over time. The following three learning rate schedules are commonly used [@book_murphy_2022]:
1. Piecewise constant: $\eta_t = \eta_i$ if $t_i \leq t \leq t_{i+1}$
2. Exponential decay: $\eta_t = \eta_0 e^{-\lambda t}$
3. Polynomial decay: $\eta_t = \eta_0 (\beta t + 1)^{-\alpha}$

In the piecewise constant schedule, $t_i$ are a set of time points at which we adjust the learning rate to a specified value. For example, we may set $\eta_i = \eta_0 \gamma^i$, which reduces the initial learning rate by a factor of $\gamma$ for each threshold that we pass. This is called *step decay*. Another approach called *time decay rate* decreases the learning for batch training within an epoch, as well as for each epoch. If $m$ denotes the number of batches and $e_i$ the current epoch, the learning rate is updated according to

$$
  \eta_i(t) = \frac{t_0}{t + t_1},\; i=1,\dots
$$

where $t = e_i m + i$ and $t_0$ and $t_1$ are fixed parameters. Sometimes the threshold times are computed adaptively, by estimating when the train or validation loss are plateaued. This is called *reduce-on-plateau*.

Exponential decay is typically too fast. A common choice is polynomial decay, with $\alpha = 0.5$ and $\beta = 1$. This corresponds to a *square-root schedule*, $\eta_t = \eta_0 \frac{1}{\sqrt{t + 1}}$.

### Stochastic Gradient Descent

Stochastic gradient descent (SGD) is a gradient descent (GD) method applied iteratively to small, randomly selected subsets of the training data, known as minibatches. This process repeated over multiple cycles called epochs.  While the theoretical rate of convergence for SGD is slower than that of traditional GD, it often achieves faster convergence in practice. The inherent randomness of SGD allows it to escape local minima in the cost function, a challenge that can trap standard GD when using a sufficiently small learning rate [@book_murphy_2022, pg. 293].

#### Adaptive Gradient (AdaGrad)

The adaptive gradient (AdaGrad) method uses an update of the following form [@book_murphy_2022, pg. 299]:

$$
\label{equation-adagrad}
  \theta_{t+1, j} = \theta_{t,p} - \eta_t \frac{1}{\sqrt{s_{t,j} + \epsilon}} g_{t,i},\; i=1,\dots,p
$$

where $s_{t,j} = \sum_{i=1}^t g_{i,j}^2$ is the sum of the squared gradients and $\epsilon > 0$ is a small term to avoid dividing by zero. Equivalently, we can write the update in the vector form

$$
\label{equation-adagrad-vector}
  \Delta\boldsymbol{\theta}_t = \eta_t \frac{1}{\sqrt{\mathbf{s}_t + \epsilon}}\mathbf{g}_t
$$

#### Root mean square propagation (RMSProp)

The root mean square propagation (RMSProp) method uses the update an exponentially moving average of the past squared gradients, rather than their sum [@book_murphy_2022, pg. 299]:

$$
  s_{t+1, j} = \beta s_{t,j} + (1 - \beta) g_{t,j}^2
$$

It is common to set $\beta \approx 0.9$, which puts more weight on recent examples. In this case, we get [@book_murphy_2022, pg. 299]

$$
  \sqrt{s_{t,j}} \approx \operatorname{RMS}(\mathbf{g}_{1:t,j}) = \sqrt{\frac{1}{t}\sum_{\tau=1}^t g_{\tau,j}^2}
$$

The overall update of the RMSProp takes the same form as [](#equation-adagrad).

#### Adaptive moment estimation (ADAM)

Adaptive moment estimation (ADAM) is another method that combines RMSProp with momentum through the intermediate updates [@book_murphy_2022, pg. 300]

$$
\begin{align*}
  \mathbf{m}_t =& \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \mathbf{g}_t \\
  \mathbf{s}_t =& \beta_2 \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2 
\end{align*}
$$

The parameters are then updated using [@book_murphy_2022, pg. 300]

$$
  \Delta\boldsymbol{\theta}_t = -\eta_t \frac{1}{\sqrt{s}_t + \epsilon} \mathbf{m}_t
$$

The standard values for the various constants are [@book_murphy_2022, pg. 300]
- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\epsilon = 10^{-6}$

If we set $\beta_1 = 0$ and no bias-correction, we recover RMSProp, which does not use momentum. For the overall learning rate, it is common to use a fixed value such as $\eta_t = 0.001$.

If we initialize with $\mathbf{m}_0 = \mathbf{s}_0 = \mathbf{0}$, then initial estimates will be biased towards small values. This can be avoided with biased-corrected moments, which increase the values early in the optimization process. These estimates are given by [@book_murphy_2022, pg. 300]

$$
\begin{align*}
  \hat{\mathbf{m}}_t =& \frac{\mathbf{m}_t}{1 - \beta_1^t} \\
  \hat{\mathbf{s}}_t =& \frac{\mathbf{s}_t}{1 - \beta_2^2}
\end{align*}
$$

## Feedforward Neural Network

An artificial neural network (ANN) is a computational graph model where the nodes, called neurons, are connected in a layered structure. The first and last layers are referred to as input and output layers, respectively, while the intermediate ones are called hidden layers. A feedforward neural network (FFNN) is a type of ANN in which the information only flows in one direction (see [](#mathematical-description-of-feedforward-neural-networks) for details).

Suppose we have an FFNN with $L\in\N$ hidden layers, each having $n_\ell$ neurons for $L\in\set{1,\dots,L}$. Assume further that the FFNN takes in an input $\mathbf{x}\in X\subseteq\mathbb{F}^P$ with $P$ features, and gives an output $\mathbb{y}\in Y\subseteq\mathbb{F}^Q$, where $\mathbb{F}$ is a field. In the case of using an FFNN as a regression model for the $2$-dimensional Franke function, the input space is $X = \R^2$, while the output space is $Y = \R$. The FFNN can be modelled as a function $\mathcal{F}: \mathbb{F}^{n_0}\to \mathbb{F}^{n_{L+1}}$ given by 

$$
  \mathcal{F} := \boldsymbol{\sigma}_{L+1} \circ \F_{L+1} \circ \boldsymbol{\sigma}_{L} \circ F_L \circ\cdots\circ \boldsymbol{\sigma}_1 \circ F_1,
$$

where $F_\ell :\mathbb{F}^{n_{\ell-1}} \to\mathbb{F}^{n_\ell}$ are affine transformations representing a forward pass from layer $\ell-1$ to $\ell$ and $\boldsymbol{\sigma}_\ell : \mathbb{F}^{n_\ell} \to \mathbb{F}^{n_\ell}$ are activation functions applied following a forward pass. In a forward pass from layer $\ell - 1$ to $\ell$, the affine transformation $F_\ell$ produces weighted pre-activations $\mathbf{z}^{(\ell)} \in \mathbb{F}^{n_\ell}$ given by

$$
  \mathbf{z}^{(\ell)} = F_i(\mathbf{a}^{(\ell - 1)}) = \mathbf{W}^{(\ell)} \mathbf{a}^{(\ell - 1)} + \mathbf{b}^{(\ell)},
$$

where
- $\mathbf{W}^{(\ell)} \in\mathbb{F}^{n_{\ell-1}\times n_{l}}$ is an $n_{\ell-1} \times n_{\ell}$ matrix representing linear weights,
- $\mathbf{b}^{(\ell)} \in\mathbb{F}^{\ell}$ is a bias vector
- $\mathbf{a}^{(\ell - 1)} = \boldsymbol{\sigma}_{\ell-1} (\mathbf{z}^{(\ell-1)})$ is the activation output from the previous layer $\ell - 1$.

### Backpropagation Algorithm

A feedforward neural network (FFNN) can be trained using stochastic gradient. To calculate the gradients for each layer iteratively, a technique called backpropagation can be used. Appendix [](#derivation-of-the-backpropagation-algorithm) outlines the derivation of the backpropagation algorithm, which can be explained in the following steps

- **Calculate the output error:** First determine the output error $\boldsymbol{\delta}^{(L+1)}$ using the gradient of the cost with respect to the activations:

$$
  \boldsymbol{\delta}^{(\ell)} = \nabla_{\mathbf{a}^{(\ell)}} C_n \odot \sigma'_{\ell} (\mathbf{z}^{(\ell)}),
$$

where $\odot$ denotes the Hadamard (elementwise) product.

- **Backpropagate the errors:** For each layer $\ell = L,L-1,\dots,1$ calculate the error $\boldsymbol{\delta}^{(\ell)}$ using

$$
  \boldsymbol{\delta}^{(\ell)} = (\mathbf{W}^{\ell + 1})^\top \boldsymbol{\delta}^{(\ell + 1)} \odot \boldsymbol{\sigma}'_{\ell} (\mathbf{z}^{(\ell)}).
$$

- **Update weights and biases:** For each layer $\ell = L,L-1,\dots,1$ adjust the biases $\mathbf{b}^{(\ell)} = (b_j^{(\ell)})_{j=1}^{n_\ell}$ and weights $\mathbf{W}^{(\ell)} = (w_{jk}^{(\ell)})_{j,k=1}^{n_{\ell-1}, n_{\ell}}$ according to the updates

$$
\begin{align*}
  \hat{w}_{jk}^{(\ell)} =& w_{jk}^{(\ell)} - \eta \delta_j^{(\ell)} a_k^{(\ell - 1)} \\
  \hat{b}_j^{(\ell)} =& b_j^{(\ell)} - \eta \delta_j^{(\ell)},
\end{align*}
$$

where $\eta$ is the learning rate.

### Activation Functions

Activation functions give artificial neural networks non-linear properties. Without them, a neural network is nothing more than a linear model, regardless of the depth. Additionally, activation functions restrict the outputs to specific ranges, which is essential depending on the application, such as for binary or multi-class classification tasks. 

This project studies the following activation functions (see [](#figure-activation_functions)):
- **Sigmoid function**: The sigmoid function $\sigma:\R\to (0,1)$, or the logistical function, is given by

$$
\begin{align*}
  \sigma(x) =& \frac{1}{1 + e^{-x}} \\
  \frac{\drm}{\drm x} \sigma(x) =& \frac{e^{-x}}{(1 + e^{-x})^2} = \sigma(x) (1 - \sigma(x))
\end{align*}
$$

This a an "S"-shaped function that output values between $0$ and $1$, and is thus suitable for representing probability. The sigmoid function suffers from the vanishing gradient problem [@notes_smets_2024, pp. 22-23]. When a sigmoid activation is close to $0$ or $1$, the gradient becomes very small due to its assymptotic behaviour. This leads to ineffective weight updates during backpropagation. 

- **Rectified linear unit (ReLU)**: The ReLU function $\operatorname{ReLU}:\R \to [0,\infty)$ is given by

$$
\begin{align*}
  \operatorname{ReLU}(x) :=& \max\set{0,x} \\
  \frac{\drm}{\drm x}\operatorname{ReLU} =& \begin{cases} 1,\quad x >& 0 \\ 0,\quad x <& 0 \end{cases}
\end{align*}
$$

The derivative of the ReLU behaves similarly to the Heaviside step function (although they are not technically identical). This property helps mitigate the vanishing gradient problem, as the derivative remains constant for positive activations, allowing gradients to propagate effectively during training. On the other hand, ReLU can lead "dying" neurons. When the input is negative, the derivative is zero, effectively deactivating the neuron. This inactivation causes the gradients of the upstream neurens to vanish [@notes_smets_2024, pp. 22-23]. 

- **Leaky rectified linear unit (LRelu)**: The leaky ReLU function $\operatorname{LReLU}:\R\times\R \to\R$ is given by
   
$$
  \operatorname{LReLU}(x,\alpha) :=& \max\set{\alpha x, x} \\
  \frac{\drm}{\drm x}\operatorname{LReLU}(x, \alpha) =& \begin{cases} 1,\quad x >& 0 \\ \alpha,\quad x <& 0 \end{cases}
$$

The leaky ReLU differs from the ordinary ReLU by introducing a small slope for negative values. This addresses the problem of "dying neurons" affecting the ReLU. This modification allows the function to maintain a small gradient for negative inputs, enabling the neuron to continue learning even when it receives negative activations.

```{figure} figures/activation_functions.pdf
:label: figure-activation_functions
:alt: Activation Functions.
:align: center

Plot of the sigmoid, rectified linear unit (ReLU) and Leaky ReLU activation functions.
```

# Results

## Linear Regression Parameter Tuning

This section presents the results of a random grid search for hyperparameter tuning of a ridge regression model. The model was trained using momentum-based gradient descent on a training set of $1,000$ samples generated by the Franke function. The detailed results are displayed in [](#table-ridge-regression-franke-hypertuning).

The tuning process employed a random grid search with the following parameters:

- Polynomial degrees: $p \sim \operatorname{Uniform}(0,10)$
- Learning rate: $\eta \sim \operatorname{Uniform}(\num{1e-4}, 0.1)$
- Regularization parameter: $\lambda \sim \operatorname{Uniform}(\num{1e-6}, 0.1)$
- Momentum parameter: $\gamma \sim\operatorname{Uniform}(\num{1e-6}, 0.1)$

The top performing models are generally of lower polynomial degree, which may suggest potential numerical instability in the momentum-based gradient descent algorithm used for training the models (see [](#code-gdm-linreg) for source code). additionally, the most effective models generally favor small learning rates on the order of $\num{1eâˆ’4}$. The regularization parameters are also generally small, but tend to fluctuate more.

:::{table} Top ten performing results from hyperparameter tuning of a ridge regression model using random grid search. The learning rate is denoted by $\eta$, the regularization parameter is denoted by $\lambda$, and the momentum parameter is denoted by $\gamma$.
:label: table-ridge-regression-franke-hypertuning
:align: center

| Degree | $\eta$ | $\lambda$ | $\gamma$ | MSE | R^2 |
| :-: | :-: | :-: | :-: | :-: | :-: |
| $0$ | $\num{1.43e-04}$ | $\num{3.55e-06}$ | $\num{2.60e-01}$ | $\num{1.10e+00} \pm \num{1.34e+00}$ | $\num{-1.22e+01} \pm \num{1.65e+01}$ |
| $0$ | $\num{1.45e-04}$ | $\num{5.13e-02}$ | $\num{1.59e-01}$ | $\num{1.87e+00} \pm \num{3.57e+00}$ | $\num{-2.22e+01} \pm \num{4.49e+01}$ |
| $1$ | $\num{1.36e-04}$ | $\num{5.36e-06}$ | $\num{1.04e-01}$ | $\num{1.89e+00} \pm \num{2.09e+00}$ | $\num{-1.98e+01} \pm \num{2.05e+01}$ |
| $0$ | $\num{1.47e-04}$ | $\num{1.22e-02}$ | $\num{4.02e-01}$ | $\num{1.95e+00} \pm \num{2.34e+00}$ | $\num{-2.14e+01} \pm \num{2.68e+01}$ |
| $0$ | $\num{1.09e-04}$ | $\num{3.65e-04}$ | $\num{5.49e-01}$ | $\num{2.19e+00} \pm \num{1.99e+00}$ | $\num{-2.53e+01} \pm \num{2.43e+01}$ |
| $1$ | $\num{1.11e-04}$ | $\num{1.16e-02}$ | $\num{3.31e-01}$ | $\num{2.36e+00} \pm \num{3.16e+00}$ | $\num{-2.85e+01} \pm \num{3.96e+01}$ |
| $2$ | $\num{1.35e-04}$ | $\num{2.57e-03}$ | $\num{1.21e-01}$ | $\num{2.58e+00} \pm \num{4.38e+00}$ | $\num{-3.03e+01} \pm \num{5.29e+01}$ |
| $1$ | $\num{1.93e-04}$ | $\num{3.02e-03}$ | $\num{6.96e-02}$ | $\num{2.84e+00} \pm \num{3.11e+00}$ | $\num{-3.32e+01} \pm \num{3.41e+01}$ |
| $0$ | $\num{2.88e-04}$ | $\num{1.87e-02}$ | $\num{5.43e-02}$ | $\num{3.06e+00} \pm \num{3.58e+00}$ | $\num{-3.54e+01} \pm \num{3.93e+01}$ |
| $0$ | $\num{1.85e-04}$ | $\num{3.55e-05}$ | $\num{5.09e-01}$ | $\num{3.51e+00} \pm \num{4.55e+00}$ | $\num{-4.19e+01} \pm \num{5.83e+01}$ |
:::

## Regression with Feedforward Neural Network

This section presents the results of employing a feedforward neural network (FFNN) for regression tasks using synthetic data generated by the Franke function. We explored various activation functions in the hidden layers, including sigmoid, ReLU, and Leaky ReLU. The performance of the FFNN was compared to linear regression methods, specifically ordinary least squares (OLS) and ridge regression. We initialized the weights of the FFNN using a scaled normal distribution based on the activation function, while the biases were set to a small constant value of $0.01$. The FFNN was configured with a constant learning rate and utilized a linear activation function for the output layer, appropriate for this regression problem.

### Parameter Tuning with Random grid Search

Parameter tuning was performed on a feedforward neural network (FFNN) using the to identify optimal layer architectures, learning rates, and $\ell_2$ regularization parameters. The tuning process employed a random grid search with the following parameters:

- Sigmoid as hidden activation function
- Learning rate: $\eta \sim \operatorname{Uniform}(\num{1e-4}, 0.1)$
- Regularization parameter: $\lambda \sim \operatorname{Uniform}(\num{1e-6}, 0.1)$
- Number of hidden layers: $L \sim \operatorname{Uniform}(1, 3)$
- Number of neurons (in powers of $2$): $2^{n_L}, n_L \sim \operatorname{Uniform}(2, 7)$

The FFNN models were trained on $1,000$ samples using $5$-fold cross-validation, with each model undergoing $100$ epochs of training and utilizing minibatches of size $32$.

[](#figure-ffnn_regression_franke_hypertuning_architecture) shows the mean squared error for various hidden layer architectures resulting from the random grid search. Overall, there are no clear trends in the performance of the FFNN models based on the number of hidden layers and neurons within each layer. This lack of discernible patterns may be partly attributed to the experimental design, as the chosen epoch size of $100$ may be insufficient for deeper FFNNs with an increasing number of neurons. Models with three hidden layers generally performed worse, suggesting a potential for overfitting; the added complexity of three layers may not be warranted for this regression task.

[](#table-ffnn-regression-franke-hypertuning-architecture) lists the score metrics for the top five performing FFNN regression models. The best model featured hidden layers of $[128, 8]$, with a learning rate of $\eta = \num{7.95e-02}$ and an $\ell_2$ regularization parameter of $\lambda = \num{4.42e-05}$. In general, the top-performing models across different depths exhibited learning rates on the order of $10^{-2}$ and regularization parameters in the range of $10^{-6}$ to $10^{-4}$

:::{table} Performance scores for the top five feedforward neural network models obtained from a random grid search parameter tuning. Here $\eta$ denotes the learning rate, and $\lambda$ denotes the $\ell_2$ regularization parameter.
:label: table-ffnn-regression-franke-hypertuning-architecture
:align: center

| Layers | Î· | Î» | MSE | R^2 | Time [s] |
| :-: | :-: | :-: | :-: | :-: | :-: |
| 128-8 | $\num{7.95e-02}$ | $\num{4.42e-05}$ | $\num{1.37e-02} \pm \num{2.74e-03}$ | $\num{8.35e-01} \pm \num{2.13e-02}$ | $\num{5.25e-01} \pm \num{5.09e-02}$ |
| 64 | $\num{9.76e-02}$ | $\num{2.16e-05}$ | $\num{1.51e-02} \pm \num{5.70e-03}$ | $\num{8.18e-01} \pm \num{5.90e-02}$ | $\num{1.78e-01} \pm \num{4.83e-03}$ |
| 16 | $\num{9.77e-02}$ | $\num{7.52e-06}$ | $\num{1.69e-02} \pm \num{3.77e-03}$ | $\num{7.95e-01} \pm \num{3.94e-02}$ | $\num{7.17e-02} \pm \num{9.03e-03}$ |
| 64-32 | $\num{8.60e-02}$ | $\num{1.16e-04}$ | $\num{1.98e-02} \pm \num{2.62e-03}$ | $\num{7.57e-01} \pm \num{4.76e-02}$ | $\num{3.06e-01} \pm \num{5.09e-03}$ |
| 16 | $\num{8.33e-02}$ | $\num{6.12e-06}$ | $\num{2.01e-02} \pm \num{7.34e-03}$ | $\num{7.59e-01} \pm \num{7.75e-02}$ | $\num{1.03e-01} \pm \num{1.64e-02}$ |
:::


```{figure} figures/ffnn_regression_franke_hypertuning_architecture.pdf
:label: figure-ffnn_regression_franke_hypertuning_architecture
:alt: Mean Squared Error of Hidden Layer Architectures for Feedforward Neural Network.
:align: center

Mean squared error for various hidden layer architectures of a feedforward neural network (FFNN) using sigmoid activation for hidden layers and linear activation for the output layer. The FFNN models were trained on $1,000$ samples using $5$-fold cross validation. The models were trained with $100$ epochs and minibatches of size $32$.
```

#### Comparison with Ordinary Least Squares and Ridge Regression

[](#table-ffnn-regression-franke-comparison) compares the performance of FFNN regression models utilizing sigmoid, ReLU, and leaky ReLU activation functions. The results are benchmarked against linear regression models and FFNNs implemented using the [Flux.jl](https://fluxml.ai/Flux.jl/stable/) package in Julia.

The mean squared error (MSE) reveals a marked difference in performance among the various hidden activation functions using (FFNN) models, particularly highlighting the ReLU and leaky ReLU (LReLU) advantages over the sigmoid activation function. The LReLU activation function achieved the lowest MSE. This superior performance can be attributed to LReLU's ability to mitagate the vanishing gradient problem, which often plagues the sigmoid function. The non-zero slope for negative inputs in LReLU enables it retain some gradient information, thereby addressing the problem of dying neurons that can affect the ReLU.

In terms of computational efficiency, both ReLU and LReLU models outpeformed the sigmoid model, suggesting a favorable balance between model performance and training time. This efficiency is particularly important in practical applications where computational resources and time are constraints.

In contrast, both ordinary least squares (OLS) and ridge regression models demonstrated even better performance while maintaining significantly higher time efficiency. This indicates that linear models may be particularly well suited for the regression task at hand. The FFNN models require a greater effort to fine-tune parameters to achieve an optimal balance between performance and computational cost.

The FFNNs implemented using the Flux.jl package performed significantly worse compared to our custom FFNN models, coupled with a substantially higher computational cost. Despite using identical training parameters for both models, the underlying reasons for this discrepancy in performance remain unclear. At the time of writing, the Flux.jl package documentation lacks comprehensive examples aimed at optimizing computational performance for such regression, often requiring a deep dive into the source code for further insights.

:::{table} Performance comparison of feedforward neural networks (FFNNs) on a regression task using different hidden activation functions. The models were configured with a layer architecture of $[2,32,16,1]$, a learning rate of $\eta = 0.05$, and a regularization parameter of $\lambda = \num{1eâˆ’5}$. Additionally, these models were trained on $1,000$ samples generated by the Franke function, employing $10$-fold cross-validation with $1,000$ epochs and a batch size of $32$. The results are benchmarked against polynomial ordinary least squares (OLS) and ridge regression models, with the OLS model featuring a polynomial degree of $14$ and the ridge model having a polynomial degree of $15$, and regularization parameter $\lambda = \num{1e-5}$. Additionally, the result are compared with FFNNs implemented using the [Flux.jl](https://fluxml.ai/Flux.jl/stable/) package in Julia.
:label: table-ffnn-regression-franke-comparison
:align: center

| Model | MSE | $R^2$ | Time [s] |
| :-: | :-: | :-: | :-: |
| FFNN (sigmoid) | $\num{4.15e-03} \pm  {2.58e-03}$ | $\num{9.51e-01}\pm\num{2.84e-02}$ | $\num{1.71e+00} \pm \num{3.11e-01}$ |
| FFNN (ReLU) | $\num{4.47e-04}\pm\num{1.48e-04}$ | $\num{9.95e-01} \pm \num{1.49e-03}$ | $\num{8.35e-01} \pm \num{1.42e-01}$ |
| FFNN (LReLU) | $\num{3.95e-04}\pm\num{2.39e-04}$ | $\num{9.95e-01} \pm \num{2.44e-03}$ | $\num{8.10e-01} \pm \num{1.53e-01}$ |
| Flux FFNN (sigmoid) | $\num{7.92e-02}\pm\num{1.30e-02}$ | $\num{2.65e-02} \pm \num{3.83e-02}$ | $\num{1.33e+01} \pm \num{7.60e+00}$ |
| Flux FFNN (ReLU) | $\num{7.64e-02} \pm \num{7.91e-03}$ | $\num{7.71e-02} \pm \num{3.26e-02}$ | $\num{1.09e+01} \pm \num{3.46e-01}$ |
| Flux FFNN (LReLU) | $\num{7.58e-02} \pm \num{1.20e-02}$ | $\num{7.12e-02} \pm \num{4.19e-02}$ | $\num{1.13e+01} \pm \num{4.91e-01}$ |
| OLS (p=14) | $\num{1.63e-04} \pm \num{4.27e-05}$ | $\num{9.98e-01} \pm \num{5.39e-04}$ | $\num{1.53e-02} \pm \num{1.52e-03}$ |
| Ridge (p=15) | $\num{1.61e-04} \pm \num{6.55e-05}$ | $\num{9.98e-01} \pm \num{7.54e-04}$ | $\num{1.52e-02} \pm \num{1.08e-03}$ |
:::

## Classification with Feedforward Neural Network

This section presents the results of employing a feedforward neural network (FFNN) for classification on the [Wisconsin breast cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). We explored various activation functions in the hidden layers, including sigmoid, ReLU, and Leaky ReLU. The performance of the FFNN was compared to logistic regression. We initialized the weights of the FFNN using a scaled normal distribution based on the activation function, while the biases were set to a small constant value of $0.01$. The FFNN was configured with a constant learning rate and utilized the sigmoid activation function for the output layer, appropriate for this binary classification problem.

### Parameter Tuning with Random Grid Search

As for the regression case, we also conducted parameter FFNN binary classification using random grid search with the following parameters:

- Sigmoid as hidden activation function
- Learning rate: $\eta \sim \operatorname{Uniform}(\num{1e-4}, 0.1)$
- Regularization parameter: $\lambda \sim \operatorname{Uniform}(\num{1e-6}, 0.1)$
- Number of hidden layers: $L \sim \operatorname{Uniform}(1, 3)$
- Number of neurons (in powers of $2$): $2^{n_L}, n_L \sim \operatorname{Uniform}(2, 7)$

[](#figure-ffnn_classification_wbcd_hypertuning_architecture) shows the accuracy score for various hidden layer architectures obtained from the random grid search. There are no strong trends observed in the scores relative to the number of hidden layers or neurons. Generally, the single-layer models beformed slightly better the two- and three-layer models. Notably, the accuracy scores for the three-layer models tend to plateau around an accuracy of approximately $0.63$. This trend is prevalent across most configurations of neurons, indicating that the three-layer models may be experiencing saturation in their learning process. This plateau could be attributed to the use of the sigmoid activation function, which is known to be prone to vanishing gradients.

[](#table-ffnn-classification-wbcd-hypertuning-architecture) lists the score metrics for the top five performing FFNN classification models. The best model featured hidden layers of $[128, 8]$, with a learning rate of $\eta = \num{6.16e-02}$ and an $\ell_2$ regularization parameter of $\lambda = \num{2.00e-06}$. The second-best model achieved slightly lower accuracy but required significantly less computational time. This illustrates a trade-off between model complexity and computational efficiency. The models with fewer layers (such as the single-layer configuration with 4 neurons) exhibited comparable accuracy, indicating that while deeper architectures can enhance performance, they may not always yield substantial gains over simpler configurations.

:::{table} Accuracy scores and computational time for the top five feedforward neural network models obtained from a random grid search parameter tuning on the [Wisconsin breast cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). Here $\eta$ denotes the learning rate, and $\lambda$ denotes the $\ell_2$ regularization parameter.
:label: table-ffnn-classification-wbcd-hypertuning-architecture
:align: center

| Layers | Î· | Î» | Accuracy | Time [s] |
| :-: | :-: | :-: | :-: | :-: |
| 128-8 | $\num{6.16e-02}$ | $\num{2.00e-06}$ | $\num{9.72e-01} \pm \num{1.45e-02}$ | $\num{3.07e-01} \pm \num{5.63e-03}$ |
| 32 | $\num{8.64e-02}$ | $\num{1.25e-06}$ | $\num{9.67e-01} \pm \num{2.09e-02}$ | $\num{7.44e-02} \pm \num{4.76e-03}$ |
| 4 | $\num{6.20e-02}$ | $\num{1.07e-05}$ | $\num{9.63e-01} \pm \num{2.09e-02}$ | $\num{2.04e-02} \pm \num{9.53e-04}$ |
| 16 | $\num{9.80e-02}$ | $\num{2.63e-05}$ | $\num{9.63e-01} \pm \num{1.56e-02}$ | $\num{4.68e-02} \pm \num{2.75e-03}$ |
| 64-32 | $\num{4.72e-02}$ | $\num{3.63e-04}$ | $\num{9.63e-01} \pm \num{7.35e-03}$ | $\num{1.96e-01} \pm \num{6.16e-03}$ |
:::

```{figure} figures/ffnn_classification_wbcd_hypertuning_architecture.pdf
:label: figure-ffnn_classification_wbcd_hypertuning_architecture
:alt: Mean Squared Error of Hidden Layer Architectures for Feedforward Neural Network.
:align: center

Accuracy score for various hidden layer architectures of a feedforward neural network (FFNN) used for binary classification. The FFNN models were configured sigmoid activation for hidden layers and sigmoid activation for the output layer. The FFNN models were trained on the [Wisconsin breast cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) using $5$-fold cross validation. The models were trained with $100$ epochs and minibatches of size $32$.
```

### Comparison with Logistic Regression

[](#table-ffnn-classification-wbcd-comparison) compares the performance of FFNN models for binary classification, utilizing sigmoid, ReLU, and leaky ReLU (LReLU) activation functions. The results are benchmarked against a logistic regression model.

Consistent with the regression findings, the FFNN models using the ReLU and leaky ReLU (LReLU) activation functions  significantly outperformed the sigmoid variant, while also incurring lower computational costs. This improvement is expected given the ability of the ReLU and LReLU to overcome the vanishing gradient problem often associated with the sigmoid function.

The logistic model performed comparatively to the ReLU and LReLU models, , but it achieved this with significantly lower computational time. This finding underscores the efficiency of logistic regression in balancing performance and computational cost, especially in simpler classification tasks.

:::{table} Performance comparison of feedforward neural networks (FFNNs) on a binary classification task using different hidden activation functions. The models were configured with a layer architecture of $[2,32,16,1]$, a learning rate of $\eta = 0.05$, and a regularization parameter of $\lambda = \num{1eâˆ’5}$. Additionally, these models were trained on the [Wisconsin breast cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic), employing $10$-fold cross-validation with $1,000$ epochs and a batch size of $32$. The results are benchmarked against a logistical regression model with learning rate $\eta = 0.05$ and $\ell_2$ regularization parameter $\lambda = \num{1e-5}$.
:label: table-ffnn-classification-wbcd-comparison
:align: center

| Model | Accuracy | Time [s] |
| :-: | :-: | :-: |
| FFNN (sigmoid) | $\num{6.29e-01} \pm \num{5.88e-02}$ | $\num{1.28e+00} \pm \num{2.40e-01}$ |
| FFNN (ReLU) | $\num{9.74e-01} \pm \num{2.39e-02}$ | $\num{7.10e-01} \pm \num{9.82e-02}$ |
| FFNN (LReLU) | $\num{9.75e-01} \pm \num{2.07e-02}$ | $\num{7.52e-01} \pm \num{1.06e-01}$ |
| LogReg | $\num{9.56e-01} \pm \num{2.79e-02}$ | $\num{1.43e-02} \pm \num{5.32e-03}$ |
:::

# Conclusion

This study demonstrates that linear models can perform comparably to more complex feedforward neural networks in simple regression and binary classification tasks. The linear models achieve this level of performance with significantly lower computational costs, avoiding the extensive iterative matrix operations associated with training neural networks via backpropagation. This efficiency not only simplifies the tuning of parameters but also highlights the practical advantages of using linear models in scenarios where computational resources and time are critical considerations.

# Appendix

## Code Repository

The Julia source code used to generate the test results is available at [https://github.com/semapheur/fys-stk4155](https://github.com/semapheur/fys-stk4155).

## Mathematical Description of Feedforward Neural Networks

An artificial neural, or a computing unit, can be modelled as an affine transformation followed by non-linear activation function. Specifically, if $\mathbf{x}\in X\subseteq\mathbb{F}^n$ is the input signal and $\mathbf{y}\in Y\subseteq\mathbb{F}^m$ is output signal, where $\mathbb{F}$ is a field, an artifical neuron takes the form:

$$
  \mathbf{y} = \sigma(\mathbf{Wx} + \mathbf{b}),
$$

where
- $\mathbf{W}\in\mathbb{F}^{m\times n}$ is an $m\times n$ matrix representing linear weights,
- $\mathbf{b}\in\mathbb{F}^m$ is a bias vector,
- $\sigma:\mathbb{F}^m \to Y$ is an activation function.

The input and output spaces can generally assume many forms depending on the application. Common forms for $Y$ include:
- the real line $\R$ for regression problems.
- binary values $\set{0,1}^n$ for classification problems.
- unit intervals $[0,1]^n$ for representing probabilities.

A feedforward neural network (FFNN) is a directed acyclic graph of artifical neurons, in which information only flows in one direction. The neurons of an FFNN are structured into a sequence of layers. In this structure, the first and last layers are called the input and output layers, respectively, while the intermediate ones are referred to as hidden layers. An FFNN with $L\in\N$ hidden layers can be modelled as a composition of affine transformations chained by activation functions. This construction can be formalized as follows. Let $n_0, n_1,\dots,n_{L+1}\in\N$ be the number of neurons in each layer and $\boldsymbol{\sigma}_1,\dots,\sigma_L$ be (vectorized) activation functions. Suppose $F_\ell :\mathbb{F}^{n_{\ell-1}} \to\mathbb{F}^{n_\ell}$ are affine transformations given by

$$
  F_\ell (\mathbf{x}) = \mathbf{W}^{(\ell)} \mathbf{x} + \mathbf{b}^{(\ell)},
$$

where $\mathbf{W}^{(\ell)} \in\mathbb{F}^{n_\ell \times n_{\ell-1}}$ and $\mathbf{b}^{(\ell)} \in\mathbb{F}^{n_\ell}$ for $\ell\in\set{1,\dots,L+1}$. The FFNN is then represented by the composition $\mathcal{F}: \mathbb{F}^{n_0} \to \mathbb{F}^{n_{L+1}}$ given by

$$
  \mathcal{F} := \boldsymbol{\sigma}_{L+1} \circ F_{L+1} \circ \boldsymbol{\sigma}_{L} \circ F_L \circ\cdots\circ \boldsymbol{\sigma}_1 \circ F_1.
$$

In a forward pass from layer $\ell - 1$ to $\ell$, the affine transformation $F_\ell$ produces weighted pre-activation $\mathbf{z}^{(\ell)} \in \mathbb{F}^{n_\ell}$ given by

$$
  \mathbf{z}^{(\ell)} = F_i(\mathbf{a}^{(\ell - 1)}) = \mathbf{W}^{(\ell)} \mathbf{a}^{(\ell - 1)} + \mathbf{b}^{(\ell)},
$$

where $\mathbf{a}^{(\ell - 1)} = \boldsymbol{\sigma}_{\ell-1} (\mathbf{z}^{(\ell-1)})$ is the activation output from the previous layer $\ell - 1$. For each layer $\ell\in{1,\dots,L+1}$, the pre-activation and activation for neuron $j$ is written

$$
\label{equation-neuron-activation}
\begin{split}
  z_j^{(\ell)} =& \sum_{i=1}^{n_{\ell-1}} w_{ji}^{(\ell)} a_i^{(\ell - 1)} + b_j^{(\ell)} \\
  a_j^{(\ell)} =& \sigma_\ell (z_j^{(\ell)}),
\end{split}
$$

where $w_{ji}^{(\ell)}$ is the weight from neuron $i$ in layer $\ell - 1$ to neuron $j$ in layer $\ell$, and $b_j^{(\ell)}$ is the bias for neuron $j$ in layer $\ell$.

### Derivation of the Backpropagation Algorithm

In this section we derive the backpropagation algorithm for training a feedforward neural netork (FFNN) using a quadratic cost function. The notation and methodology follow the framework established in [@book_nielsen_2015] and [@notes_smets_2024].

Suppose we have a training set $\set{(\mathbf{x}_n, \mathbf{y}_n) | \mathbf{x}\in\mathbb{F}^{n_0}, \mathbb{F}^{n_{L+1}}}_{n=1}^N$ of $N\in\N$ samples. We are interested in minimizing the mean squared error given by

$$
\label{equation-ffnn-cost-mse}
  C(\mathbf{W}, \mathbf{b}) = \frac{1}{2N} \sum_{n=1}^N \lVert \hat{\mathbf{y}}_n - \mathbf{y} \rVert^2,
$$

where $\hat{\mathbf{y}} = \mathbf{a}^{(L+1)} = \sigma(\mathbf{z}^{(L+1)})$ is the output of FFNN for the $n$th input. The cost of training example $n$, given by

$$
\label{equation-ffnn-cost-mse-example}
  C_n (\mathbf{W}, \mathbf{b}) = \frac{1}{2} \lVert \mathbf{a}^{(L+1)} - \mathbf{y}_n \rVert^2 = \frac{1}{2} \sum_{j=1}^{n_{L+1}} (a_j^{(L+1)} - y_n)^2
$$

To derive the backpropagation equations, we need to calculate the partial derivatives of $C_n$ with respect to the bias and the weights. From [](#equation-neuron-activation), we obtain the partial derivatives

$$
  \frac{\partial z_j^{(\ell)}}{\partial w_{kj}^{(\ell)}} = a_k^{(\ell - 1)},\quad
  \frac{\partial z_j^{(\ell)}}{\partial b_j^{(\ell)}} = 1,\quad
  \frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} = \sigma'_{\ell}(\mathbf{z}_j^{\ell})
$$

We also define the error $\delta_j^{(\ell)}$ for neuron $j$ in layer $\ell$ by 

$$
  \delta_j^{(\ell)} := \frac{\partial C_n}{\partial z_j^{(\ell)}} = \frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{(\ell)}} = \sigma'_\ell (z_j^{(\ell)}) \frac{\partial C_n}{\partial a_j^{(\ell)}} 
$$

In matrix form, this can be written as

$$
  \boldsymbol{\delta}^{(\ell)} = \nabla_{\mathbf{a}^{(\ell)}} C_n \odot \sigma'_{\ell} (\mathbf{z}^{(\ell)})
$$

where $\odot$ denotes the Hadamard (elementwise) product. For the output layer $\ell = L + 1$ we find from [](#equation-ffnn-cost-mse), that $\nabla_{\mathbf{a}^{(L+1)}} C_n = \mathbf{a}^{(L)} - \mathbf{y}_n$ leading to the output error

$$
  \boldsymbol{\delta}^{(L)} = (\mathbf{a}^{(L)} - \mathbf{y}_n) \odot \sigma'_{L+1} (\mathbf{z}^{(L)})
$$

For any hidden layer $\ell < L + 1$, the partial derivative of $C_n$ with respect to an activation $a_k^{(\ell)}$ is found by summing up the errors $\delta_j^{(\ell + 1)}$ of the next layer $\ell + 1$, i.e.,

$$
\label{equation-neuron-error-hidden}
  \frac{\partial C_n}{\partial a_k^{(\ell)}} = \sum_{j=1}^{n_{\ell+1}} \underbrace{\frac{\partial z_j^{(\ell + 1)}}{\partial a_k^{(\ell)}}}_{=w_{kj}^{(\ell+1)}} \underbrace{\frac{\partial a_j^{(\ell + 1)}}{\partial z_j^{(\ell+1)}} \frac{\partial C_n}{\partial a_j^{(\ell+1)}}}_{=\delta_j^{(\ell+1)}} = \sum_{j=1}^{n_{\ell+1}} w_{kj}^{(\ell + 1)} \delta_{j}^{(\ell+1)}.
$$

In matrix form this can be expressed as $\nabla_{\mathbf{a}^{(\ell)}} C_n = (\mathbf{W}^{(\ell+1)})^\top \boldsymbol{\delta}^{(\ell+1)}$. Thus, for any hidden layer $\ell < L + 1$, the error $\boldsymbol{\delta}^{(\ell)}$ can be expressed as

$$
  \boldsymbol{\delta}^{(\ell)} = (\mathbf{W}^{\ell + 1})^\top \boldsymbol{\delta}^{(\ell + 1)} \odot \boldsymbol{\sigma}'_{\ell} (\mathbf{z}^{(\ell)}).
$$

#### Gradient Computation

It remains to calculate the partial derivatives of the cost $C_n$ with respect to the bias and the weights. For any layer $\ell$, the partial derivative of $C_n$ with respect to a weight $w_{kj}^{(\ell)}$ is given by the chain rule

$$
  \frac{\partial C_n}{\partial w_{kj}^{(\ell)}} = \underbrace{\frac{\partial z_j^{(\ell)}}{\partial w_{kj}^{(\ell)}}}_{=a_k^{(\ell-1)}} \underbrace{\frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{\ell}}}_{=\delta_j^{(\ell)}} = a_k^{(\ell-1)} \delta_j^{(\ell)}
$$

Likewise, the partial derivative of $C_n$ with respect a bias $b_j^{(\ell)}$ is given by

$$
  \frac{\partial C_n}{\partial b_j^{(\ell)}} = \underbrace{\frac{\partial z_j^{(\ell)}}{\partial b_j^{(\ell)}}}_{=1} \underbrace{\frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{\ell}}}_{=\delta_j^{(\ell)}} = \delta_j^{(\ell)}
$$

The backpropagation algorithm can be summed up as follows:
- **Find the output error:** Calculate the output error $\boldsymbol{\delta}^{(L+1)}$ using the gradient of the cost with respect to the activations:

$$
  \boldsymbol{\delta}^{(\ell)} = \nabla_{\mathbf{a}^{(\ell)}} C_n \odot \sigma'_{\ell} (\mathbf{z}^{(\ell)}).
$$

- **Backpropagate the errors:** For each layer $\ell = L,L-1,\dots,1$ calculate the error $\boldsymbol{\delta}^{(\ell)}$ using

$$
  \boldsymbol{\delta}^{(\ell)} = (\mathbf{W}^{\ell + 1})^\top \boldsymbol{\delta}^{(\ell + 1)} \odot \boldsymbol{\sigma}'_{\ell} (\mathbf{z}^{(\ell)}).
$$

- **Update weights and biases:** For each layer $\ell = L,L-1,\dots,1$ adjust the biases $\mathbf{b}^{(\ell)}$ and weights $\mathbf{W}^{(\ell)}$ according to the updates

$$
\begin{align*}
  \hat{w}_{jk}^{(\ell)} =& w_{jk}^{(\ell)} - \eta \delta_j^{(\ell)} a_k^{(\ell - 1)} \\
  \hat{b}_j^{(\ell)} =& b_j^{(\ell)} - \eta \delta_j^{(\ell)},
\end{align*}
$$

where $\eta$ is the learning rate.

## Franke's Function

In this project, Franke's function was used to generate training data for the polynomial regression models. This is a two-dimensional scalar field $f:\R^2 \to\R$ given by a weighted sum of four exponentials:

$$
\label{equation-12}
\begin{split}
  f(x,y) =& \frac{3}{4} \exp\left(-\frac{(9x - 2)^2}{4} - \frac{(9y - 2)^2}{4} \right) \\
  &+ \frac{3}{4}\exp\left(\frac{(9x + 1)^2}{49} - \frac{9y + 1}{10}\right) \\
  &+ \frac{1}{2}\exp\left(-\frac{(9x - 7)^2}{4} - \frac{(9y - 3)^2}{4} \right) \\
  &- \frac{1}{5}\exp\left(-(9x - 4)^2 - (9y - 7)^2 \right)
\end{split}
$$

A plot of the Franke function on the unit square $[0,1]^2$ is given in [](#figure-franke).

```{figure} figures/franke.svg
:label: figure-franke
:alt: Franke function
:align: center

Plot of the Franke function on the unit square $[0,1]^2$.
```

## Source Code

```{code} julia
:label: code-gdm-linreg
:caption: Julia implementation of gradient descent for linear regression models using momentum.

"""
Linear Regression with Gradient Descent Momentum

This function performs linear regression with gradient descent
and momentum.

Arguments:
- `X::Matrix{Float64}`: The design matrix of the model.
- `y::Vector{Float64}`: The target vector of the model.
- `iter::Int`: The number of iterations to perform.
- `momentum_parameter::Float64`: The momentum parameter. Defaults to 0.
- `l2_lambda::Float64`: The regularization parameter. Defaults to 0.
- `learning_rate::Union{Float64,Nothing}`: The learning rate. If `nothing`, it is set to 1 / max eigenvalue of the hessian.

Returns:
- The estimated coefficients of the model.
- The loss at each iteration.
"""
function linreg_gradient_descent_momentum(
  X::Matrix{Float64},
  y::Vector{Float64},
  iter::Int,
  momentum_parameter::Float64=0.0,
  l2_lambda::Float64=0.0,
  learning_rate::Union{Float64,Nothing}=Nothing,
)
  n, m = size(X)

  if isnothing(learning_rate)
    hessian = (2.0 / n) * X' * X + 2.0 * l2_lambda * I
    eigenvalues = eigvals(hessian)
    learning_rate = 1.0 / maximum(eigenvalues)
  end

  Î² = randn(m)
  momentum = zeros(m)

  losses = Vector{Float64}()
  for _ = 1:iter
    gradient = ((2.0 / n) * X' * (X * Î² - y) + 2.0 * l2_lambda * Î²)
    momentum = momentum_parameter * momentum - learning_rate * gradient

    Î² -= momentum
    prediction = X * Î²
    loss = mean((prediction .- y) .^ 2)
    push!(losses, loss)
  end

  return Î², losses
end
```

```{code} julia
:label: code-ffnn
:caption: Feedforward neural network implementation in Julia.

"""
Layer(weights, biases, activation, activation_derivative)

A struct representing a layer in a neural network.

# Fields
- `weights::Matrix{Float64}`: A matrix of weights for the layer.
- `biases::Vector{Float64}`: A vector of biases for the layer.
- `activation::Function`: The activation function for the layer.
- `activation_prime::Function`: The derivative of the activation function for the layer.
"""
struct Layer
  weights::Matrix{Float64}
  biases::Vector{Float64}
  activation::Function
  activation_prime::Function
end

"""
NeuralNetwork

A struct representing a neural network.

# Fields
- `layers::Vector{Layer}`: A vector of layers in the network.
- `cost::Function`: The cost function used to evaluate the network.
- `cost_prime::Function`: The derivative of the cost function used to evaluate the network.
- `lr_scheduler::LearningRateScheduler`: The learning rate scheduler used to update the network.
- `l2_lambda::Float64`: The regularization strength for L2 regularization.
"""
struct NeuralNetwork
  layers::Vector{Layer}
  cost::Function
  cost_prime::Function
  lr_scheduler::LearningRateScheduler
  l2_lambda::Float64
end
```

```{code} julia
:label: code-ffnn-constructor
:caption: Julia FFNN constructor.

"""
Initialize a neural network with specified architecture and parameters. The neural network is initialized using the input convention (samples, features).

# Parameters
- `layer_sizes::Vector{Int}`: A vector specifying the number of neurons in each layer.
- `activation::Function`: The activation function for the hidden layers.
- `activation_prime::Function`: The derivative of the activation function.
- `cost::Function`: The cost function for evaluating the network.
- `cost_prime::Function`: The derivative of the cost function.
- `lr_scheduler::LearningRateScheduler`: Scheduler for adjusting the learning rate (default: ConstantLR(0.01)).
- `l2_lambda::Float64`: L2 regularization strength (default: 0.0).

# Returns
- `NeuralNetwork`: An initialized neural network with the given specifications.
"""
function initialize_network(
  layer_sizes::Vector{Int},
  hidden_activation::Function,
  hidden_activation_prime::Function,
  output_activation::Function,
  output_activation_prime::Function,
  cost::Function,
  cost_prime::Function,
  lr_scheduler::LearningRateScheduler,
  l2_lambda::Float64=0.0,
)::NeuralNetwork
  layers = Vector{Layer}()

  # Add hidden layers
  for i = 1:(length(layer_sizes)-2)
    fan_in = layer_sizes[i] # Number of inputs to the layer
    fan_out = layer_sizes[i+1] # Number of outputs from the layer

    scale = initialization_scale(hidden_activation, fan_in)

    weights = randn(fan_in, fan_out) * scale
    biases = zeros(fan_out) .+ 0.01
    push!(layers, Layer(weights, biases, hidden_activation, hidden_activation_prime))
  end

  # Add output layer
  fan_in = layer_sizes[end-1]
  fan_out = layer_sizes[end]

  scale = initialization_scale(output_activation, fan_in)

  weights = randn(fan_in, fan_out) * scale
  biases = zeros(fan_out) .+ 0.01
  push!(layers, Layer(weights, biases, output_activation, output_activation_prime))

  return NeuralNetwork(layers, cost, cost_prime, lr_scheduler, l2_lambda)
end
```

```{code} julia
:label: code-ffnn-forwardpass
:caption: Neural forward pass implementation in Julia.

"""
Performs a forward pass through the neural network.

# Parameters
- `network::NeuralNetwork`: The neural network to perform the forward pass on.
- `input::Matrix{Float64}`: The input matrix to the network.

# Returns
- `Tuple{Vector{Matrix{Float64}}, Vector{Matrix{Float64}}}`: A tuple containing activations and weighted inputs at each layer.
"""
function feedforward(
  network::NeuralNetwork,
  input::Matrix{Float64},
)::Tuple{Vector{Matrix{Float64}},Vector{Matrix{Float64}}}
  activations = Vector{Matrix{Float64}}(undef, length(network.layers) + 1)
  weighted_inputs = Vector{Matrix{Float64}}(undef, length(network.layers))

  activations[1] = input

  for (i, layer) in enumerate(network.layers)
    weighted_input = activations[i] * layer.weights .+ layer.biases'
    weighted_inputs[i] = weighted_input
    activations[i+1] = layer.activation.(weighted_input)
  end

  return activations, weighted_inputs
end
```

```{code} julia
:label: code-backpropagation
:caption: Backpropagation implementation in Julia.

"""
Performs backpropagation on a neural network to update its weights and biases.

# Parameters
- `network::NeuralNetwork`: The neural network to update.
- `input::VectorMatrix{Float64}`: The input to the network.
- `target_output::Vector{Float64}`: The target output of the network.
- `learning_rate::Float64`: The learning rate to use for the update.

# Returns
- `Float64`: The cost of the network for the given input and target output.
"""
function backpropagate(
  network::NeuralNetwork,
  input::Matrix{Float64},
  target_output::Matrix{Float64},
  learning_rate::Float64,
)
  # Forward pass
  activations, weighted_inputs = feedforward(network, input)

  # Output layer error
  error =
    network.cost_prime(activations[end], target_output) .*
    network.layers[end].activation_prime.(weighted_inputs[end])

  # Initialize gradients
  weight_gradients = [zeros(size(layer.weights)) for layer in network.layers]
  bias_gradients = [zeros(size(layer.biases)) for layer in network.layers]

  # Backpropagate errors
  for l in reverse(1:(length(network.layers)))
    bias_gradients[l] = vec(sum(error, dims=1))
    weight_gradients[l] =
      activations[l]' * error + (network.l2_lambda .* network.layers[l].weights)

    if l > 1
      error =
        (error * network.layers[l].weights') .*
        network.layers[l-1].activation_prime.(weighted_inputs[l-1])
    end
  end

  # Update weights and biases
  for l = 1:(length(network.layers))
    network.layers[l].weights .-= learning_rate * weight_gradients[l]
    network.layers[l].biases .-= learning_rate * bias_gradients[l]
  end

  return network.cost(activations[end], target_output)
end
```

```{code} julia
:label: code-ffnn-sgd
:caption: Julia implementation of FFNN training using stochastic gradient descent.

"""
Get batches of data for training a neural network.

# Parameters
- `X::Matrix{Float64}`: The design matrix containing the inputs to the network.
- `y::Vector{Float64}`: The target outputs of the network.
- `batch_size::Int`: The size of each batch.

# Returns
- `Vector{Tuple{Matrix{Float64},Vector{Float64}}}`: The batches, each as a tuple of the design matrix and target output.
"""
function get_batches(x::Matrix{Float64}, y::Matrix{Float64}, batch_size::Int)
  n_samples = size(x, 1)
  indices = shuffle(1:n_samples)

  n_batches = ceil(Int, n_samples / batch_size)
  batches = Vector{Tuple{Matrix{Float64},Matrix{Float64}}}()

  for i = 1:n_batches
    start_idx = (i - 1) * batch_size + 1
    end_idx = min(i * batch_size, n_samples)
    batch_indices = indices[start_idx:end_idx]
    push!(batches, (x[batch_indices, :], y[batch_indices, :]))
  end

  return batches
end

"""
Train a neural network using backpropagation and stochastic gradient descent.

# Parameters
- `network::NeuralNetwork`: The neural network to train.
- `X::Matrix{Float64}`: The design matrix containing the inputs to the network.
- `y::Matrix{Float64}`: The target outputs of the network.
- `epochs::Int`: The number of epochs to train the network for.
- `learning_rate::Float64`: The learning rate to use for stochastic gradient descent.

# Returns
- `Vector{Float64}`: A vector of the loss at each epoch.
"""
function train_network(
  network::NeuralNetwork,
  x::Matrix{Float64},
  y::Matrix{Float64},
  epochs::Int,
  batch_size::Int,
  verbose::Bool=true,
)::Vector{Float64}
  losses = Vector{Float64}()

  for epoch = 1:epochs
    current_lr = network.lr_scheduler(epoch - 1)

    epoch_loss = 0.0

    batches = get_batches(x, y, batch_size)

    for (batch_x, batch_y) in batches
      batch_loss = backpropagate(network, batch_x, batch_y, current_lr)
      epoch_loss += batch_loss * size(batch_x, 1)
    end

    push!(losses, epoch_loss / n_samples)

    if verbose && (epoch % 100 == 0 || epoch == 1)
      println("Epoch $epoch/$epochs: loss = $(losses[end])")
    end
  end

  return losses
end
```

```{code} julia
:label: code-logreg
:caption: Logistic regression implementation in Julia.

mutable struct LogisticRegression
  learning_rate::Float64
  l2_lambda::Float64
  num_iterations::Int
  beta_logreg::Union{Nothing,Vector{Float64}}

  function LogisticRegression(learning_rate, l2_lambda, num_iterations)
    new(learning_rate, l2_lambda, num_iterations, nothing)
  end
end

function train_logistic_regression!(model::LogisticRegression, X, y)
  n_data, num_features = size(X)
  model.beta_logreg = zeros(Float64, num_features)

  for _ = 1:model.num_iterations
    linear_model = X * model.beta_logreg
    y_predicted = sigmoid.(linear_model)

    # Gradient calculation with l2 regularization
    gradient = (X' * (y_predicted - y)) / n_data .+ model.l2_lambda * model.beta_logreg

    # Update beta_logreg
    model.beta_logreg -= model.learning_rate * gradient
  end
end

function predict_logistic_regression(
  model::LogisticRegression,
  X::Matrix{Float64},
)::Vector{Float64}
  linear_model = X * model.beta_logreg
  y_predicted = sigmoid.(linear_model)
  return [i >= 0.5 ? 1.0 : 0.0 for i in y_predicted]
end
```