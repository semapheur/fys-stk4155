---
title: Linear Regression
subtitle: FYS-STK4155 - Project 1
authors:
  - name: Insert Name
site:
  template: article-theme
exports:
  - format: pdf
    template: ../report_template
    output: report.pdf
    showtoc: true
math:
  # Note the 'single quotes'
  '\argmin': '\operatorname{argmin}'
  '\R': '\mathbb{R}'
  '\Set': '{\left\{ #1 \right\}}'
bibliography: references.bib
abstract: |
  This project studies the predicitve performance of 2D polynomial models trained using ordinary least squares (OLS), ridge regression, and lasso regression, applied to both synthetic data generated by a noisy Franke function and real digital elevation data. Our analysis reveals that OLS is prone to overfitting, particularly for higher-degree models. This overfitting behavior is highlighted in the observed bias-variance tradeoff. In contrast, both ridge and lasso regression demonstrate the ability to mitigate overfitting for higher-degree models, attributed to their regularization constraints that effectively shrink the regression coefficients of higher-degree terms. Lasso regression also performs variable selection by setting some coefficients to zero. However, our experiments did not provide conclusive evidence that ridge regression or lasso improves overall model performance compared to OLS.
---

# Introduction

This project studies the properties of various linear regression methods. We will use ordinary least squares (OLS) regression, ridge regression and lasso regression to fit two-dimensional polynomial models to both synthetically generated data and real data sets. First we will apply the methods to train polynomial models on data generated by Franke's equation. This will be used evaluate the predictive performance of the models as a function of complexity. Finally, we apply the methods to train and evaluate polynomial models on digital elevation data.

# Theory and Method

The following theoretical overview of linear regression is based on @murphy2022probabilisticmachinelearning, @hastie2009elementsofstatisticallearning, and @vanwieringen2023lecturenotesridgeregression.

## Linear Regression

Consider a statistical experiment with an input variable $\mathbf{x}\in X\subseteq \R^p$ of $p$ covariates producing a real response $y\in Y\subseteq \R$. Taking $n$ samples of the experiment produces a labeled training set

$$
  S = \Set{(\mathbf{x}_i, y_i) \in X\times Y | X\subseteq \R^p, Y\subseteq\R }_{i=1}^n
$$

The aim of regression analysis is predict future responses using some function $f: X\to\R$ such that $f(\mathbf{x}_i) \approx y_i$ for each $i = 1,\dots,n$  by solving an optimization algorithm applied on the training set $S$. Linear regression assumes a linear relationship between $X$ and $Y$ that is subject to Gaussian noise. This linearity is modeled through an error variable $\epsilon\sim\mathcal{N}(0,\sigma^2)$ having zero-mean Gaussian distribution with variance $\sigma^2$. For each sample $i=1,\dots,n$, the model takes the form

$$
  \label{equation-1}
  \mathbf{y}_i = \left(\sum_{j=0}^p \mathbf{x}_{ij} \beta_j \right) + \epsilon_i = \mathbf{x}_i^\top \boldsymbol{\beta} + \epsilon_i
$$

where $\boldsymbol{\beta} = \left[\begin{smallmatrix} \beta_0 & \dots & \beta_p \end{smallmatrix}\right]^\top$ is the *regression parameter*. Each regression coefficient $\beta_j$ measures the effect of a unit change in covariate vector $x_{ij}$, while keeping all other covariates fixed. Furthermore, each $\epsilon_i$ is assumed to be independent, i.e. $\operatorname{cov}(\epsilon_i, \epsilon_j) = 0$ for $i\neq j$.

The linear regression model can be written more compactly in the matrix form

$$
  \label{equation-2}
  \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$

where $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}_n, \sigma^2 \mathbf{I}_n)$ and

$$
  \mathbf{X} = \begin{bmatrix}
    1 & x_{11} & \cdots & x_{1p} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n1} & \cdots & x_{np}
  \end{bmatrix}
$$

The $n\times(p+1)$-matrix $\mathbf{X}$ is called the *design matrix*. Equation [](#equation-2) shows that the model is linear in $\boldsymbol{\beta}$. This means that the design matrix $\mathbb{X}$ can be fitted to $\mathbf{y}$ using non-linear functions $f$, such as polynomials.

The dependence on the Gaussian noise term $\boldsymbol{\epsilon}$ implies that $\mathbf{y}$ is a random variable with Gaussian distribution. The expected value of $y_i$ is given by

$$
\begin{align*}
  \mathbb{E}(y_i) =& \mathbb{E}\left(\epsilon_i + \sum_{j=0}^p \mathbf{x}_{ij} \beta_j  \right) = \underbrace{\mathbb{E}(\epsilon_i)}_{=0} + \sum_{j=0}^p \mathbb{E}(x_{ij} \beta_j) = \sum_{j=0}^p x_{ij} \beta_j = \mathbf{X}_{i,*} \boldsymbol{\beta}
\end{align*}
$$

which follows from linearity of the expected value, and the fact that $\mathbf{X}\boldsymbol{\beta}$ is deterministic (non-random). The variance of $y_i$ is given by

$$
\begin{align*}
  \operatorname{var}(y_i) =& \mathbb{E}([y_i - \mathbb{E}(y_i)^2]) = \mathbb{E}(y_i^2) - \mathbb{E}(y_i)^2 \\
  =& \mathbb{E}[(\mathbf{X}_{i,*}\boldsymbol{\beta})^2 + 2\epsilon_i \mathbf{X}_{i,*}\boldsymbol{\beta} + \epsilon_i^2] \\
  =& \mathbb{E}[(\mathbb{X}_{i,*}\boldsymbol{\beta})^2] + 2\mathbb{E}(\epsilon_i \mathbf{X}_{i,*}\boldsymbol{\beta}) + \mathbb{E}(\epsilon_i^2) - (\mathbf{X}_{i,*} \boldsymbol{\beta})^2 \\
  =& (\mathbf{X}_{i,*} \boldsymbol{\beta})^2 + 2\mathbf{X}_{i,*}\boldsymbol{\beta}\underbrace{\mathbb{E}(\epsilon_i)}_{=0} - (\mathbf{X}_{i,*} \boldsymbol{\beta})^2 \\
  =& \mathbb{E}(\epsilon_i^2) = \operatorname{var}(\epsilon_i) = \sigma^2
\end{align*}
$$

This shows that $y_i \sim\mathcal{N}(\mathbf{X}_{i,*}\boldsymbol{\beta}, \sigma^2)$ has a Gaussian distribution with mean $\mathbf{X}_{i,*}\boldsymbol{\beta}$ and variance $\sigma^2$. The probability density function (PDF) for $y_i$ is given by

$$
  \label{equation-3}
  g(y_i|\mathbf{X}\boldsymbol{\beta}, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp[-\frac{(y_i - \mathbf{X}_{i,*} \boldsymbol{\beta})^2}{2\sigma^2}]
$$

## Ordinary Least Squares (OLS)

The regression parameter $\boldsymbol{\beta}$ can be estimated using maximum likelihood estimation (MLE). Assuming that $y_i \sim\mathcal{N}(\mathbf{X}_{i,*}\boldsymbol{\beta}, \sigma^2)$ are independent and identically distributed, we can define the likelihood function

$$
\label{equation-4}
  L(\boldsymbol{\beta}, \sigma^2) = \prod_{i=1}^n g(y_i|\mathbf{X}\boldsymbol{\beta}, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \prod_{i=1}^n \exp\left(-\frac{1}{2\sigma^2}(y_i - \mathbf{X}_{i,*} \boldsymbol{\beta})^2\right)
$$

taking the natural logarithm gives the log-likelihood function

$$
\label{equation-5}
\begin{split}
  \ln[L(\boldsymbol{\beta}, \sigma^2)] =& \sum_{i=1}^n \left(\frac{1}{\sqrt{2\pi}\sigma} \exp\left[-\frac{1}{2\sigma^2} (y_i - \mathbf{X}_{i,*} \boldsymbol{\beta})^2 \right]\right) \\ 
  =& -n \ln(\sqrt{2\pi}\sigma) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mathbf{X}_{i,*} \boldsymbol{\beta})^2 \\
  =& -n \ln(\sqrt{2\pi}\sigma) - \frac{1}{2\sigma^2} \lVert \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \rVert_2^2, 
\end{split}
$$

where $\lVert \mathbf{x} \rVert_2 = \left(\sum_{i=1}^n x_i^2 \right)^{1/2} = \sqrt{\mathbf{x}^\top \mathbf{x}}$ is the $\ell_2$-norm. The term $\lVert \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \rVert_2^2$ is known as the residual sum of squares.

To find the MLE of $\boldsymbol{\beta}$, we need to solve the maximization problem

$$
  \hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}} L(\boldsymbol{\beta}, \sigma^2) \iff \hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}} \ln[L(\boldsymbol{\beta}, \sigma^2)],
$$

where the equivalence follows from the strict monotonicity of the natural logarithm. Since only the residual sum of squares term of [](#equation-5) depends on $\boldsymbol{\beta}$, this is equivalent to minimizing the cost function

$$
  \label{equation-6}
  C(\boldsymbol{\beta}) = \frac{1}{n} \lVert \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \rVert_2^2
$$

The gradient of [](#equation-6) with respect to $\boldsymbol{\beta}$ is

$$
\begin{align*}
  \nabla_{\boldsymbol{\beta}} C(\boldsymbol{\beta}) =& \frac{1}{n} \nabla_{\boldsymbol{\beta}} (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\
  =& \frac{1}{n} \nabla_{\boldsymbol{\beta}} (\mathbf{y}^\top \mathbf{y} - 2\mathbf{y}^\top \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}) \\
  =& \frac{1}{n}(\mathbf{0} - 2\mathbf{X}^\top \mathbf{y} + 2\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}) \\
  =& \frac{2}{n} \mathbf{X}^\top (\mathbf{X}\boldsymbol{\beta} - \mathbf{y})
\end{align*}
$$

Solving for $\nabla_{\boldsymbol{\beta}} = \mathbf{0}$ gives the normal equation

$$
  \mathbf{X}^\top \mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\top \mathbf{y}
$$

which in turn gives the MLE

$$
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} = \mathbf{X}^+ \mathbf{y},
$$

where $\mathbf{X}^+ = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top$ is the Moore-Penrose inverse or pseudoinverse of $\mathbf{X}$.

The expected value of $\hat{\boldsymbol{\beta}}$ is given by

$$
\begin{align*}
  \mathbb{E}(\hat{\boldsymbol{\beta}}) = \mathbb{E}\left[\mathbf{X}^+ \mathbf{y}\right] = \mathbf{X}^+ \mathbf{y} \mathbb{E}(\mathbf{y}) = \mathbf{X}^+ \mathbf{X} \boldsymbol{\beta} = \boldsymbol{\beta}
\end{align*}
$$

and the variance of $\hat{\boldsymbol{\beta}}$ is given by

$$
\begin{align*}
  \operatorname{var}{\hat{\boldsymbol{\beta}}} =& \mathbb{E}\left([\hat{\boldsymbol{\beta}} - \mathbb{E}(\hat{\boldsymbol{\beta}})][\hat{\boldsymbol{\beta}} - \mathbb{E}(\hat{\boldsymbol{\beta}})]^\top \right) = \mathbb{E}\left([\mathbf{X}^+ \mathbf{y} - \boldsymbol{\beta}][\mathbf{X}^+ \mathbf{y} - \boldsymbol{\beta}]^\top \right) \\
  =& \mathbf{X}^+ \mathbb{E}(\mathbf{y}\mathbf{y}^\top)\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} - \boldsymbol{\beta}\boldsymbol{\beta}^\top \\
  =& \mathbf{X}^+ (\mathbf{X}\boldsymbol{\beta}\boldsymbol{\beta}^\top \mathbf{X}^\top + \sigma^2 \mathbf{I}_n) \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} - \boldsymbol{\beta}\boldsymbol{\beta}^\top \\
  =& \boldsymbol{\beta}^\top \boldsymbol{\beta} + \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} - \boldsymbol{\beta}\boldsymbol{\beta}^\top \\
  =& \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}
\end{align*}
$$

There are two main problems with maximum likelhood estimation using ordinary least squares: collinearity and overfitting. Collinearity occurs when two or more covariates are strongly linearly correlated. This makes it hard to discern the effect of collinear covariates because their impact may be intertwined. Overfitting occurs when the regression model fits to the observed noise rather than the underlying pattern. This may result in a model that generalizes poorly to novel data.

```{figure} figures/ols_geometry.svg
:label: figure-1
:alt: Geometric interpretation of OLS estimator.
:align: center

Geometric intepretation of the ordinary least squares estimator.
```

The problem of collinearlity and overfitting can be mitigated with several techniques, of which regularization and cross-validation are examined in this report. Regularization involves adding a constraint to the likelihood function that penalizes large norms of the regression parameter. The two regularization methods examined in this report are ridge regression and lasso regression, where LASSO stands for "least shrinkage and selection operator". Ridge regression adds a penalty based on the square $\ell_2$-norm of the regression coefficients and is therefore also called $\ell_2$ regularization. Lasso regression, on the other hand, adds a penalty based on the $\ell_1$-norm of the coefficients and is also called $\ell_1$ regularization.

## Ridge Regression ($\ell_2$ Regularization)

Ridge regression adds a regularization to the likelihood function [](#equation-6) with a penalty to the square $\ell_2$-norm of the regression coefficients:

$$
\label{equation-7}
  C_{\text{ridge}} (\boldsymbol{\beta}, \lambda) = \frac{1}{n}\lVert\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\rVert_2^2 + \lambda\lVert\boldsymbol{\beta}\rVert = \sum_{i=1}^n (y_i - \mathbf{X}_{i,*}\boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2
$$

The penalty term, $\lambda\lVert\boldsymbol{\beta}\rVert_2^2$ is called the *ridge penalty* and $\lambda\in[0,\infty)$ is called the *regularization term*. When $\lambda = 0$, the ridge cost function reduces to the OLS cost function [](#equation-6).

To find the ridge estimator of $\boldsymbol{\beta}$, we need to solve $\nabla_{\boldsymbol{\beta}} C_{\text{ridge}} (\boldsymbol{\beta}, \lambda) = \mathbf{0}$. Ignoring the $1/n$ scalar, the gradient of [](#equation-7) with respect to $\boldsymbol{\beta}$ is

$$
\begin{split}
  \nabla_{\boldsymbol{\beta}} C_{\text{ridge}} (\boldsymbol{\beta}, \lambda) =& \nabla_{\boldsymbol{\beta}} \left(\lVert \mathbf{y} - \mathbf{X}\boldsymbol{\beta}\rVert_2^2 + \lambda\lVert\boldsymbol{\beta}\rVert_2^2 \right) \\
  =& 2\mathbf{X}^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + 2\lambda \mathbf{I}_p \boldsymbol{\beta} = 2(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I}_p)\boldsymbol{\beta} - 2\mathbf{X}^\top \mathbf{y}
\end{split}
$$

Solving $\nabla_{\boldsymbol{\beta}} C_{\text{ridge}} = \mathbf{0}$ gives the ridge estimator

$$
  \hat{\boldsymbol{\beta}}(\lambda) = (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I}_p)^{-1} \mathbf{X}^\top \mathbf{y}
$$

As outlined in @vanwieringen2023lecturenotesridgeregression [pg.9], we can apply a singular value decomposition (SVD) $\mathbf{X} = \mathbf{USV}^\top$, where
- $\mathbf{V}^\top \mathbf{V} = \mathbf{VV}^\top = \mathbf{I}_p$
- $\mathbf{UU}^\top \mathbf{U} = \mathbf{UU}^\top = \mathbf{I}_n$
- $\mathbf{S} = \operatorname{diag}(s_{ii})_{i=1}^{\min(n,p)}$ is a diagonal $n\times p$ matrix with singular values on the main diagonal.
  
In terms of this decomposition, the ridge estimator takes the form

$$
\label{equation-8}
  \hat{\boldsymbol{\beta}}(\lambda) = \mathbf{V}(\mathbf{S}^\top \mathbf{S} + \lambda\mathbf{I}_p)^{-1} \mathbf{S}^\top \mathbf{U}^\top \mathbf{y}
$$

The SVD form of the OLS estimator is

$$
\label{equation-9}
  \hat{\boldsymbol{\beta}} = \mathbf{V}(\mathbf{S}^\top \mathbf{S})^{-1} \mathbf{S}^\top \mathbf{U}^\top \mathbf{y}
$$

Note that for the diagonal $n\times p$-matrix $\mathbf{S} = \operatorname{diag}(s_{ii})_{i=1}^p$, we have $\mathbf{S}^\top \mathbf{S} = \operatorname{diag}(s_{ii}^2)_{i=1}^p$ and $(\mathbf{S}^\top \mathbf{S})^{-1} = \operatorname{diag}(1/s_{ii}^2)_{i=1}^p$. Consequently, we get 

$$
\begin{align*}
  (\mathbf{S}^\top \mathbf{S})^{-1} \mathbf{S} =& \operatorname{diag}(1/s_{ii})_{i=1}^{\min(n,p)} \\
  (\mathbf{S}^\top \mathbf{S} + \lambda\mathbf{I}_p)^{-1} \mathbf{S} =& \operatorname{diag}(s_{ii}/(s_{ii}^2 + \lambda))_{i=1}^{\min(n,p)}
\end{align*}
$$ 

Comparing $(\mathbf{S}^\top \mathbf{S})^{-1} \mathbf{S}$ in [](#equation-8) to $(\mathbf{S}^\top \mathbf{S} + \lambda\mathbf{I}_p)^{-1} \mathbf{S}$ in [](#equation-9), we see that

$$
  \frac{1}{s_{ii}} \geq \frac{s_{ii}}{s_{ii}^2 + \lambda}
$$

This inequality shows that the ridge penalty effectively shrinks the singular values of the OLS estimation, which consequently results in a reduction of the regression coefficients.

## Lasso Regression ($\ell_1$ Regularization)

Lasso regression adds a regularization to the cost function [](#equation-5) with a penalty to the $\ell_1$-norm of the regression coefficients:

$$
  C_{\text{lasso}} (\boldsymbol{\beta}, \lambda) = \frac{1}{n}\lVert\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\rVert_2^2 - \lambda\lVert\boldsymbol{\beta}\rVert_1 = \sum_{i=1}^n (y_i - \mathbf{X}_{i,*}\boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p |\beta_j|,
$$

Since the $\ell_1$ norm $\lVert\boldsymbol{\beta}\rVert_1$ is not differentiable at $\beta = \mathbf{0}$, there are generally no analytical expressions for the lasso estimation of $\boldsymbol{\beta}$. However, an analytic expression exists for orthonormal design matrices, i.e. $\mathbf{X}^\top \mathbf{X} = \mathbf{I}_p$. In this case, the maximum likelihood estimate is given by $\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{y}$. Using this we can rewrite the lasso cost function as

$$
\begin{align*}
  \lVert \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \rVert_2^2 + \lambda\lVert\beta\rVert_1 =& \mathbf{y}^\top \mathbf{y} - \mathbf{y}^\top \mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{y} + \boldsymbol{\beta}\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta} + \lambda \sum_{j=1}^p |\beta_j| \\
  =& -\hat{\boldsymbol{\beta}} \boldsymbol{\beta} - \boldsymbol{\beta}^\top \hat{\boldsymbol{\beta}} + \boldsymbol{\beta}^\top \boldsymbol{\beta} + \lambda \sum_{j=1}^p |\beta_j| \\
  =& \sum_{j=1}^p -2\hat{\beta}_j \beta_j + \beta_j^2 + \lambda |\beta_j|
\end{align*}
$$

The lasso regression estimation can be found by solving the following minimization problem for each regression coefficient:

$$
  \arg\min_{\beta_j} -2\hat{\beta}_j \beta_j + \beta_j^2 + \lambda|\beta_j| = \begin{cases}
    \arg\min_{\beta_j} -2\hat{\beta}_j \beta_j + \beta_j^2 + \lambda \beta_j, \quad& \beta_j > 0 \\
    \arg\min_{\beta_j} -2\hat{\beta}_j \beta_j + \beta_j^2 - \lambda \beta_j, \quad& \beta_j < 0
  \end{cases}
$$

Taking the partial derivative with respect to $\beta_j$ gives

$$
  \frac{\partial}{\partial\beta_j} 2\hat{\beta}_j \beta_j + \beta_j^2 \pm \lambda \beta_j = -\hat{\beta}_j + 2\beta_j - \lambda
$$

Equation this to $0$ and solving for $\hat{\beta}_j$ gives

$$
  \hat{\beta}_j (\lambda_1) = \begin{cases}
    \hat{\beta_j} - \lambda/2, \quad& \hat{\beta}_j (\lambda) > 0 \\
    0,\quad& \hat{\beta}(\lambda) = 0 \\
    \hat{\beta_j} + \lambda/2, \quad& \hat{\beta}_j (\lambda) < 0
  \end{cases}
$$

which can be written more compactly as

$$
  \hat{\beta_j} (\lambda_i) = \operatorname{sign}(\hat{\beta}_j) \left(|\hat{\beta}_j| - \frac{\lambda}{2}\right)_+ =: \operatorname{SoftThreshold}\left(\hat{\beta}_j, \frac{\lambda}{2}\right),
$$

where $x_+ = \max(0, x)$ is the positive part of $x$. This shows that the lasso regression estimation applies a threshold to the OLS estimation.

In addition to shrinking regression coefficients, lasso regression can also perform variable selection by setting the corresponding coefficient to zero. To see this, the lasso optimization problem can be reformulated as a constrained estimation problem, where the penalty term imposes a bound on the $\ell_1$​-norm of the regression coefficients. This penalty creates a diamond-shaped constraint surface, with its vertices aligned along the coordinate axes (see [](#figure-2)). 

The lasso estimator is defined as the point $\hat{\boldsymbol{\beta}}(\lambda)$ within this constraint surface that minimizes the residual sum of squares. If this optimal point coincides with one of the vertices of the diamond, it indicates that the corresponding regression coefficient is driven to zero, effectively eliminating the corresponding variable.

```{figure} figures/l1_l2_norms.pdf
:label: figure-2
:alt: Comparison of l_1 and l_2 norms
:align: center

Comparison of the the unit $\ell_1$ and $\ell_2$ norms in $\R^2$.
```
As outlined in @vanwieringen2023lecturenotesridgeregression [pp. 112-117], there are several numerical methods for evaluating the lasso regression estimator. This project implements the coordinate descent method, which minimizes the lasso cost function along one coordinate at time.

## Resampling Methods

### Bias-Variance Tradeoff

To derive the bias-variance tradeoff, we rewrite the OLS cost function [](#equation-5) with $\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}$ as a mean squared error

$$
  C(\boldsymbol{\beta}) = \frac{1}{n} \lVert \mathbf{y} - \hat{\mathbf{y}} \rVert = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 = \mathbb{E}[(\mathbf{y} - \hat{\mathbf{y}})^2]
$$

We assume an underlying noisy model $\mathbf{y} = \mathbf{f} + \boldsymbol{\varepsilon}$, where $\mathbf{f} = \mathbf{f}(\mathbf{x}_0)$ is the true function value for some input $\mathbf{x}_0$ and $\boldsymbol{\varepsilon} \sim\mathcal{N}(\mathbf{0},\sigma^2 \mathbf{I}_n)$ is independent, zero-mean Gaussian noise. We can expand the mean square error in the following way:

$$
\label{equation-10}
\begin{split}
  \mathbb{E}\left[(\mathbf{y} - \hat{\mathbf{y}})^2\right] =& \mathbb{E}[(\mathbf{f} + \boldsymbol{\varepsilon} - \hat{\mathbf{y}})^2] = \mathbb{E}[((\mathbf{f} - \hat{\mathbf{y}}) + \boldsymbol{\varepsilon}^2)] \\
  =& \mathbb{E}[(\mathbf{f} - \hat{\mathbf{y}})^2 + 2\boldsymbol{\varepsilon}(\mathbf{f} - \hat{\mathbf{y}}) + \boldsymbol{\varepsilon}^2] \\
  =& \mathbb{E}[(\mathbf{f} - \hat{\mathbf{y}})^2] + 2\underbrace{\mathbb{E}(\boldsymbol{\varepsilon})}_{=0}\mathbb{E}(\mathbf{f} - \hat{\mathbf{y}}) + \underbrace{\mathbb{E}(\boldsymbol{\varepsilon}^2)}_{=\operatorname{var}(\boldsymbol{\varepsilon})} \\
  =& \mathbb{E}[(\mathbf{f} - \hat{\mathbf{y}})^2] + \sigma^2
\end{split}
$$

where we have used the fact that $\boldsymbol{\varepsilon}$ is independent from the $\mathbf{f} - \hat{\mathbf{y}}$, which in turn is deterministic. Expanding the first expected value by adding and subtracting $\mathbb{E}(\hat{\mathbf{y}})$ we get

$$
\label{equation-11}
\begin{split}
  \mathbb{E}\left[(\mathbf{f} - \hat{\mathbf{y}})^2\right] =& \mathbb{E}\left[([\mathbf{f} - \mathbb{E}(\hat{\mathbf{y}})] + [\mathbb{E}(\hat{\mathbf{y}}) - \hat{\mathbf{y}}])^2\right] \\
  =& \mathbb{E}\left[(\mathbf{f} - \mathbb{E}(\hat{\mathbf{y}}))^2\right] + 2\mathbb{E}\left[(\mathbf{f} - \mathbb{E}(\hat{\mathbf{y}}))(\mathbb{E}(\hat{\mathbf{y}}) - \hat{\mathbf{y}})\right] \\
  &+ \mathbb{E}\left[(\mathbb{E}(\hat{\mathbf{y}}) - \hat{\mathbf{y}})^2\right]
\end{split}
$$

Expanding the cross-term, we find

$$
\begin{align*}
  2\mathbb{E}\left[(\mathbf{f} - \mathbb{E}(\hat{\mathbf{y}}))(\mathbb{E}(\hat{\mathbf{y}}) - \hat{\mathbf{y}})\right] =& \mathbb{E}\left[\mathbf{f}\mathbb{E}(\hat{\mathbf{y}})\right] - \mathbb{E}\left[\mathbf{f}\hat{\mathbf{y}}\right] - \mathbb{E}\left[\mathbb{E}(\hat{\mathbf{y}})^2\right] + \mathbb{E}\left[\mathbb{E}(\hat{\mathbf{y}})\mathbf{f}\right] \\
  =& \mathbb{f}\mathbb{E}(\hat{\mathbb{y}}) - \mathbb{f}\mathbb{E}(\hat{\mathbf{y}}) - \mathbb{E}(\hat{\mathbf{y}})^2 + \mathbb{E}(\hat{\mathbf{y}})^2 \\
  =& 0
\end{align*}
$$

where we have used the fact that 

$$
\begin{equation*}
  \mathbb{E}\left[\mathbf{f}\mathbb{E}(\hat{\mathbf{y}})\right] = \mathbb{E}(\mathbf{f})\mathbb{E}\left[\mathbb{E}(\hat{\mathbf{y}})\right] = \mathbf{f}\mathbb{E}(\hat{\mathbf{y}})
\end{equation*}
$$

Inserting [](#equation-11) back into [](#equation-10) and using the fact that $\mathbb{E}\left[(\mathbb{E}(\hat{\mathbf{y}}) - \hat{\mathbf{y}})^2\right] = \mathbb{E}\left[(\hat{\mathbf{y}} - \mathbb{E}(\hat{\mathbf{y}}))^2\right] = \operatorname{var}(\hat{\mathbf{y}})$ (since variance is symmetric) we get

$$
\begin{align*}
  \mathbb{E}\left[(\mathbf{f} - \hat{\mathbf{y}})^2\right] =& \mathbb{E}\left[(\mathbf{f} - \mathbb{E}(\hat{\mathbf{y}}))^2\right] + \mathbb{E}\left[(\mathbb{E}(\hat{\mathbf{y}}) - \hat{\mathbf{y}})^2\right] + \sigma^2 \\
  =& \operatorname{bias}(\hat{\mathbf{y}}) - \operatorname{var}(\hat{\mathbf{y}}) + \sigma^2
\end{align*}
$$

The bias term, $\operatorname{bias}(\hat{\mathbf{y}}) = \mathbb{E}\left[(\mathbf{f} - \hat{\mathbf{y}})^2\right]$, 
is the squared difference between the true function value $f(\mathbf{x}_0)$ and the mean predicted value $\mathbb{E}(\hat{\mathbf{y}}) = \mathbb{E}(\hat{f}(\mathbf{x}_0))$ for a given input $\mathbf{x}_0$. A model with high bias tends to be too simplistic, capturing only the general trend of the data while ignoring important patterns. This leads to underfitting, where the model performs poorly on both the training and testing datasets.

The variance term, $\operatorname{var}(\hat{\mathbf{y}}) = \mathbb{E}\left[(\mathbb{E}(\hat{\mathbf{y}}) - \hat{\mathbf{y}})^2\right]$ measures the spread of the model predictions $\hat{\mathbf{y}}$. A model with high variance is sensitive to the sampling noise, rather than the underlying patterns. This leads to overfitting, where the model performs well on the training data but poorly on unseen data, as it fails to generalize.

The $\operatorname{var}[\varepsilon] = \sigma^2 \geq 0$ term represents irreducible error, due to the inherent noise in the data. This error cannot be reduced through model optimization and regularization, and represents a lower bound on the expected prediction error.

The bias-variance tradeoff suggests that there is a balance between model complexity and expected prediction error. 
When we increase model complexity, we typically see a decrease in bias. The model can better approximate the true relationship within the data. However, this comes at the cost of increased variance, as the model may become overly sensitive to noise in the training data. Conversely, if we simplify the model, bias may increase because the model may not capture the underlying patterns adequately. However, variance tends to decrease, resulting in a model that is less sensitive to changes in the test data.

## Data Generation Using Franke's Function

In this project, Franke's function was used to generate training data for the polynomial regression models. This is a two-dimensional scalar field $f:\R^2 \to\R$ given by a weighted sum of four exponentials:

$$
\label{equation-12}
\begin{split}
  f(x,y) =& \frac{3}{4} \exp\left(-\frac{(9x - 2)^2}{4} - \frac{(9y - 2)^2}{4} \right) \\
  &+ \frac{3}{4}\exp\left(\frac{(9x + 1)^2}{49} - \frac{9y + 1}{10}\right) \\
  &+ \frac{1}{2}\exp\left(-\frac{(9x - 7)^2}{4} - \frac{(9y - 3)^2}{4} \right) \\
  &- \frac{1}{5}\exp\left(-(9x - 4)^2 - (9y - 7)^2 \right)
\end{split}
$$

A plot of the Franke function on the unit square $[0,1]^2$ is given in [](#figure-3). We used two approaches to generate training data sets using [](#equation-12). One approach was to apply the function for standard uniformly distributed input values. With $n$ input samples, this generates training data of the form

$$
  S = \Set{((x_i, y_i), f(x_i, y_i)) : x_i, y_i \sim \operatorname{Uniform}(0,1)}_{i=1}^n
$$

Here the standard uniform distribution is used to confine the input values to the unit square $[0,1]^2$. Another approach was to apply the function on linearly spaced input values on the unit square $[0,1]^2$. Additionally, we modified the Franke function by adding normally distributed noise:

$$
  \hat{f}(x, y) = f(x, y) + \varepsilon,\; \varepsilon\sim\mathcal{N}(0,\sigma^2),
$$

The two given methods ensure that the input values are confined to the range $[0,1]$. Additionally, we standardized the input data to have zero mean and unit variance prior to calculating the polynomial design matrix. This enforces a single scale on the input features, which can improve numerical stability of the regression algorithms. 

As a final preprocessing step, we split the generated data into a training set applied on the regression models and a test set used for model validation. We used a train/test ratio of 0.8 throughout our experiments and applied random shuffling of the data before doing the split. This is especially important when using linearly spaced input variables to generate data with the Franke function. As seen from [](#figure-1), the Franke function creates distinct regions of elevations and depressions. Without shuffling we risk creating train and test sets that are consistenly different, which may result in a model that does not generalize well.

```{figure} figures/franke.svg
:label: figure-3
:alt: Franke function
:align: center

Plot of the Franke function on the unit square $[0,1]^2$.
```

## Regression Model Evaluation

In this project, we evaluated the performance of the trained regression models by applying them to the split test set, which allowed us to generate predicted values $\hat{\mathbf{y}} = \mathbf{X}_{\text{test}}\hat{\boldsymbol{\beta}}$. The models' expected prediction error was assessed using two key metrics: Mean Squared Error (MSE) and the $R^2$ score. For a test set with $n$ data points, the MSE is defined as

$$
  \operatorname{MSE}(\mathbf{y}, \hat{y}) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

where $y_i$ represents the observed values and $\hat{y}_i$ denote the predicted values for each data point $\mathbf{x}_i$. MSE quantifies the average squared difference between the observed and predicted values, providing a measure of the model's accuracy; lower MSE values indicate better model performance.

The $R^2$ score, also know as the coefficient of determination, is given defined as

$$
  R^2 (\mathbf{y}, \hat{\mathbf{y}}) = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
$$

where $\bar{y} = \frac{1}{n}\sum_{i=0}^n y_i$ is the mean of the observed test responses $\mathbf{y}$. The $R^2$ score measures the proportion of variance in the dependent variable that is predictable from the independent variables. An $R^2$ value closer to 1 indicates that a larger proportion of variance is explained by the model, while values closer to 0 suggest a poor fit.

We used bootstrap resampling to calculate both the MSE and $R^2$ score of the polynomial modals. Bootstrap resampling involves repeatedly drawing samples from the sample test set with replacement. This allows us observe the relative frequency of the regression parameter $\hat{\boldsymbol{\beta}}$ in order estimate its underlying probability distribution.

We also examined $k$-fold cross-validation in our regression analysis. In this method, the dataset is partitioned into $k \leq n$ equally sized folds. The model is trained on $k−1$ folds and validated on the remaining fold, a process that is repeated $k$ times, with each fold serving as the validation set once. This approach ensures that each sample is represented in both the training and test sets across the different splits. 

# Results

## Polynomial Regression of the Franke Function

The section presents the results of polynomial regression conducted on data samples samples generated with the Franke function.

### Ordinary Least Squares

#### Bootstrap Resampling

Polynomial regression using ordinary least squares was performed in experiments with datasets of $100$ and $10,000$ samples, respectively, generated by a noisy Franke function. The results from these experiments are shown in [](#figure-4).

In the $100$-sample case with a noise root variance of $0.05$, model performance improves with increasing polynomial degree up to $5$, after which performance tends to deteriorate. This indicates that while higher degrees initially capture the underlying patterns better, they eventually lead to overfitting. This effect becomes particularly pronounced at a noise variance of $0.5$, where the mean squared error rises sharply for polynomial degrees above $6$, demonstrating the model's sensitivity to noise and its tendency to overfit. 

In the setting with noise root variance of $0.5$, we also observe negative $R^2$ scores for higher order polynomials, further indicating signifiant overfitting. A negative $R^2$ indicates that the model performs worse than a simple mean of the observed data, implying that the higher-degree models are fitted to noise rather than the underlying trend.

In the $100$-sample case, the point at which higher-order polynomials begin to deteriorate seems to coincide with the number of regression coefficients becoming comparable to the sample size. For instance, a $7$-degree bivariate polynomial has $\binom{9}{7}=36$ regression coefficients, which is nearly half of the $80$ training points available in this scenario.

In the $10,000$-sample case, we do not observe any deterioration in performance for polynomial degrees up to $8$, indicating that the sample size is sufficient to support more complex models. When the noise amplitude is increased to $0.5$, we note that this primarily results in elevated mean squared error, but does not significantly impact the training process. This outcome is expected, as increased noise introduces higher irreducible error, which cannot be mitigated by merely increasing model complexity.

[](#figure-5) illustrates the regression coefficients obtained from polynomial ordinary least squares regression for degrees up to five. The coefficients exhibit similar values across the various degrees, indicating a degree of stability in the model's parameter estimates as the polynomial complexity increases.

Interestingly, the fourth and fifth-degree polynomials display identical values for the coefficients $\beta_i$​ where $i=10,\dots,14$. This suggests that the addition of higher-order terms beyond degree four does not contribute significantly to the model, reinforcing the idea of potential overfitting when utilizing more complex models.

Among the coefficients, $\beta_2$ corresponding to $x_2$,​ stands out with a notably strong negative value. This indicates a significant inverse relationship between the corresponding predictor variable and the response variable.

```{figure} figures/ols_franke_bootstrap_mse_r2.svg
:label: figure-4
:alt: MSE and R^2 for polynomial OLS regression of the Franke function
:align: center

Mean squared error (MSE) and $R^2$ scores for polynomial ordinary least squares regression on data generated by a noisy Franke function, assessed across various sample sizes and noise standard deviations. The results are based on $100$ bootstrap resamples.
```

```{figure} figures/ols_franke_coefficients.svg
:label: figure-5
:alt: Polynomial OLS regression coefficients for the Franke function using bootstrap resampling
:align: center

Regression coefficients from polynomial ordinary least squares regression applied to $10,000$ samples generated by a noisy Franke function with a standard deviation of $0.1$. The vertical lines indicate the number of coefficients corresponding to each polynomial degree. The results are based on $100$ bootstrap resamples.
```

#### $k$-Fold Cross-Validation

[](#figure-6) presents the results of $10$-fold cross-validation for polynomial regression using ordinary least squares applied to data generated by a noisy Franke function. The findings are consistent with those observed in the bootstrap resampling approach. However, cross-validation appears to be more sensitive to noise in the $100$-sample case. Specifically, the mean squared error (MSE) increases significantly with higher-degree polynomials, indicating a greater susceptibility to overfitting in the presence of noise.

```{figure} figures/ols_franke_bootstrap_mse_r2.svg
:label: figure-6
:alt: MSE and R^2 for polynomial OLS regression of the Franke function using cross-validation
:align: center

Mean squared error (MSE) and $R^2$ scores for polynomial ordinary least squares regression on data generated by a noisy Franke function, assessed across various sample sizes and noise standard deviations. The results are based on $10$-fold cross-validation.
```

#### Bias-Variance Tradeoff

[](#figure-7) shows bias-variance tradeoff for polynomial ordinary least squares regression applied to data generated by a noisy Franke function. For the case with $100$ samples, we observe that the test error consistently increases with polynomial degree, regardless of the noise level. In contrast, with $10,000$ samples, the test error remains closely aligned with the training error as model complexity increases. However, this trend shifts when the noise root variance is raised to $1.0$, indicating a more pronounced effect of noise on model performance.

```{figure} figures/ols_franke_bootstrap_bias_variance.svg
:label: figure-7
:alt: Bias-variance tradeoff for polynomial ordinary least squares regression for the Franke function
:align: center

Bias-variance tradeoff for ordinary least squares regression applied to data generated by a noisy Franke function, evaluated across various sample sizes and noise standard deviations. The results are derived from $100$ bootstrap resamples.
```

### Ridge Regression

#### Bootstrap Resampling

[](#figure-8) presents the mean squared error and $R^2$ scores for polynomial ridge regression applied to data generated by a noisy Franke function, evaluated across a range of regularization parameters $\lambda$. Notably, when $\lambda$ is below $1,000$, the performance metrics closely resemble those observed in ordinary least squares regression, indicating minimal regularization effects. However, as $\lambda$ exceeds $100,000$, the performance of higher-degree polynomial models deteriorates, suggesting that excessive regularization may hinder their ability to capture the underlying data structure. This trend underscores the importance of selecting an appropriate regularization parameter to balance bias and variance in polynomial ridge regression.

[](#figure-9) displays the regression coefficients derived from polynomial ridge regression with a regularization parameter $λ=1,000$ for polynomial degrees up to five. Notably, the coefficients are an order of magnitude lower than those obtained from ordinary least squares (OLS) regression. This significant reduction highlights the impact of regularization in constraining coefficient values. Furthermore, the ridge regression coefficients exhibit more extreme values compared to the OLS coefficients, which predominantly cluster around zero with only a few exhibiting large magnitudes.

```{figure} figures/ridge_franke_bootstrap_mse_r2.svg
:label: figure-8
:alt: MSE and R^2 for polynomial ridge regression of the Franke function
:align: center

Mean squared error and $R^2$ scores for polynomial ridge regression on $10,000$ samples generated by a noisy Franke function having a standard deviation of $0.5$. The results are based on $100$ bootstrap resamples.
```

```{figure} figures/ridge_franke_bootstrap_coefficients.svg
:label: figure-9
:alt: Polynomial ridge regression coefficients for the Franke function
:align: center

Regression coefficients from polynomial ridge regression with a regularization parameter $\lambda=1,000$ applied to $10,000$ samples generated by a noisy Franke function having a standard deviation of $0.1$. The vertical lines indicate the number of coefficients corresponding to each polynomial degree. The results are based on $100$ bootstrap resamples.
```

#### $k$-Fold Cross-Validation

[](#figure-10) shows the results of $10$-fold cross-validation for polynomial ridge regression applied to data generated by a noisy Franke function. The findings are consistent with those observed in the bootstrap resampling approach.

```{figure} figures/ridge_franke_bootstrap_mse_r2.svg
:label: figure-10
:alt: MSE and R^2 for polynomial ridge regression of the Franke function using cross-validation
:align: center

Mean squared error (MSE) and $R^2$ scores for polynomial ridge regression on data generated by a noisy Franke function, assessed across various sample sizes and noise standard deviations. The results are based on $10$-fold cross-validation.
```

### Lasso Regression

#### Bootstrap Resampling

[](#figure-11) presents the mean squared error and $R^2$ scores for polynomial lasso regression applied to data generated by a noisy Franke function, evaluated across a range of regularization parameters $\lambda$. Similar to the behavior observed in ridge regression, when $\lambda$ is below $100$, the performance metrics closely resemble those observed in ordinary least squares regression, indicating minimal regularization effects. When $\lambda$ ranges between $500$ and $1000$, lower-degree polynomial models shows improved performance relative to their higher-degree counterparts, indicating a beneficial effect of regularization in these cases. However, once $\lambda$ exceeds $2000$, model performance begins to decline significantly, indicating over-regularization.

[](#figure-12) displays the regression coefficients derived from polynomial lasso regression with a regularization parameter $λ=750$ for polynomial degrees up to five. Notably, only a few coefficients exhibit significant values, with the majority remaining near or equal to zero. This sparsity in the coefficient is attributed to the selection property of the lasso estimator.

```{figure} figures/lasso_franke_bootstrap_mse_r2.svg
:label: figure-11
:alt: MSE and R^2 for polynomial lasso regression of the Franke function
:align: center

Mean squared error and $R^2$ scores for polynomial lasso regression on $10,000$ samples generated by a noisy Franke function with standard deviation of $0.5$. The results are based on $10$ bootstrap resamples.
```

```{figure} figures/lasso_franke_bootstrap_coefficients.svg
:label: figure-12
:alt: Polynomial lasso regression coefficients for the Franke function
:align: center

Regression coefficients from polynomial lasso regression with a regularization parameter $\lambda=750$ applied to $10,000$ samples generated by a noisy Franke function a standard deviation of $0.1$. The vertical lines indicate the number of coefficients corresponding to each polynomial degree. The results are based on $100$ bootstrap resamples.
```

#### $k$-Fold Cross-Validation

[](#figure-13) shows the results of $10$-fold cross-validation for polynomial lasso regression applied to data generated by a noisy Franke function. The findings are consistent with those observed in the bootstrap resampling approach.

```{figure} figures/lasso_franke_bootstrap_mse_r2.svg
:label: figure-13
:alt: MSE and R^2 for polynomial lasso regression of the Franke function using cross-validation
:align: center

Mean squared error (MSE) and $R^2$ scores for polynomial ridge regression on data generated by a noisy Franke function, assessed across various sample sizes and noise standard deviations. The results are based on $10$-fold cross-validation.
```

### OLS, Ridge and Lasso Comparison

[](#figure-14) presents a comparison of the mean squared error (MSE) and $R^2$ scores for polynomial ordinary least squares (OLS) regression, ridge regression, and lasso regression, evaluated using $10$-fold cross-validation on $100$ samples generated from a noisy Franke function with a noise root variance of $0.1$. The optimal values for MSE and $R^2$, along with the corresponding polynomial degrees, are summarized in [](#table-1).

Ordinary least squares achieves the lowest mean squared error overall. However, the MSE score of lasso regression falls within the standard error interval. The relatively large standard deviations in both MSE and $R^2$ indicate considerable variability in performance across the different folds of cross-validation.

Each regression method reaches optimal MSE at different polynomial degrees. OLS attains the lowest error for a polynomial of degree $5$, while higher-degree models tend to overfit the data. Ridge regression achieves its lowest error with a polynomial of degree $7$, which can be attributed to the regularization effect that mitigates overfitting for higher-degree models. In contrast, lasso regression performs best with a third-degree polynomial, maintaining consistent performance as the polynomial degree increases. This can be explained from the shrinkage and selection property of the lasso regression estimator.

```{figure} figures/ols_ridge_lasso_franke_cross_validation_mse_r2.svg
:label: figure-14
:alt: MSE and R^2 for polynomial OLS, ridge and lasso regression of the Franke function using cross-validation
:align: center

Mean squared error (MSE) and $R^2$ scores for polynomial OLS, ridge and lasso regression on data generated by a noisy Franke function with $100$ samples and noise standard deviation $0.1$. The results are based on $10$-fold cross-validations.
```

:::{table} Performance evaluation of polynomial ordinary least squares (OLS), ridge regression, and lasso regression, based on $100$ samples generated from a noisy Franke function with a noise standard deviation of $0.1$. The evaluation was conducted using $10$-fold cross-validation. Mean squared errors (MSE) and $R^2$ values are presented along with their standard deviations.
:label: table-1
:align: center

| Model | Degree | MSE | $R^2$ |
| --- | :-: | :-: | :-: |
| OLS | $5$ | $\num{1.87e-02} \pm \num{8.65e-03}$ | $\num{6.79e-01} \pm \num{2.56e-01}$ |
| Ridge ($\lambda=100$) | $7$ | $\num{4.30e-02} \pm \num{2.34e-02}$ | $\num{4.40e-01} \pm \num{2.58e-01}$ |
| Lasso ($\lambda=1$) | $3$ | $\num{2.36e-02} \pm \num{1.63e-02}$ | $\num{6.58e-01} \pm \num{2.11e-01}$ |
:::

## Polynomial Regression of Digital Elevation Data

This section present the results of polynomial regression conducted on samplings from digital elevation data (see [](#figure-15)). 

```{figure} figures/srtm_data_norway.png
:label: figure-15
:alt: SRTM elevation data for Norway
:align: center

Digital elevation data over Norway used for polynomial regression analysis.
```

[](#figure-16) shows a comparison of the mean squared error (MSE) and $R^2$ scores for polynomial ordinary least squares (OLS) regression, ridge regression, and lasso regression, evaluated using $10$-fold cross-validation on $100$ samples of digital elevation data. The optimal values for MSE and $R^2$, along with the corresponding polynomial degrees, are summarized in [](#table-2).

```{figure} figures/ols_ridge_lasso_terrain_cross_validation_mse_r2.svg
:label: figure-16
:alt: MSE and R^2 for polynomial OLS, ridge and lasso regression of digital elevation data using cross-validation
:align: center

Mean squared error (MSE) and $R^2$ scores for polynomial OLS, ridge and lasso regression on $100$ samples from digital elevation data. The results are based on $10$-fold cross-validations.
```

The polynomial models generally performed poorly due to the inherent noise in elevation data. Lasso regression achieved the lowest mean squared error overall. However, the MSE score of OLS regression falls within the standard error interval. 

As observed with the Franke-generated data, each regression method reaches its optimal MSE at different polynomial degrees. OLS has lowest error with a polynomial of degree $2$, while higher-degree polynomials tend to overfit the data. Ridge regression with $\lambda = 100$ achieves its lowest error with a polynomial of degree $3$. Its performance remains consistent for higher polynomial degrees. Lasso regression with $\lambda = 1,000$ performs best with a seventh-degree polynomial, maintaining consistent performance as the polynomial degree increases.

:::{table} Performance evaluation of polynomial ordinary least squares (OLS), ridge regression, and lasso regression, based on $100$ samples of digital terrain data. The evaluation was conducted using $10$-fold cross-validation. Mean squared errors (MSE) and $R^2$ values are presented along with their standard deviations.
:label: table-2
:align: center
 
| Model | Degree | MSE | $R^2$ |
| --- | :-: | :-: | :-: |
| OLS | $2$ | $\num{4.73e+04} \pm \num{1.86e+04}$ | $\num{3.83e-01} \pm \num{2.95e-01}$ |
| Ridge ($\lambda=100$) | $3$ | $\num{1.41e+05} \pm \num{5.64e+04}$ | $\num{-9.36e-01} \pm \num{1.28e+00}$ |
| Lasso ($\lambda=1,000$) | $7$ | $\num{4.63e+04} \pm \num{1.37e+04}$ | $\num{3.77e-01} \pm \num{2.29e-01}$ |
:::

### Bias variance tradeoff

[](#figure-17) shows bias-variance tradeoff for polynomial ordinary least squares regression applied to digital elevation data. For the case with $100$ samples, we observe that the test error consistently increases with polynomial degree. In contrast, with $10,000$ samples, the test error remains closely aligned with the training error as model complexity increases.

```{figure} figures/ols_terrain_bootstrap_bias_variance.svg
:label: figure-17
:alt: Bias-variance tradeoff for polynomial ordinary least squares regression for digital elevation data
:align: center

Bias-variance tradeoff for ordinary least squares regression applied to digital elevation data, evaluated across various sample sizes. The results are derived from $100$ bootstrap resamples.
```

# Conclusion

This project has explored the performance of polynomial models trained with ordinarly least squares (OLS) regression, ridge regression and lasso regression applied on both synthetic data generated by a noisy Franke function and digital elevation data. 

Our findings indicate that ordinary least squares (OLS), as a bias-free estimator, is prone to overfitting when applied to higher-degree polynomials. This behavior is evident in the bias-variance tradeoff observed in our analysis. In contrast, ridge regression and lasso regression show a tendency to mitigate overfitting for higher degree polynomials. This is attributed to the constraints they impose on the regression parameters through regularization, which shrink the coefficients of higher-degree terms. This has the effect of shrinking regression coefficients for higher degree terms. Additionally, lasso regression not only shrinks coefficients but can also set some to zero, thereby performing variable selection. 

Our experiments did not provide evidence that ridge regression and lasso regression improve overall model performance compared to OLS. This suggests that while regularization techniques offer theoretical advantages, their practical effectiveness may depend on the specific characteristics of the data and the chosen model complexity.

# Appendix

## Code Repository

The Julia source code used to generate the test results is available at [https://github.com/semapheur/fys-stk4155](https://github.com/semapheur/fys-stk4155).

## Source Code

### Ordinary Least Squares
```{code} julia
:label: code-1
:caption: Ordinary least squares regression in Julia.

"""
Ordinary least squares

Implements the ordinary least squares algorithm using the normal equation or
the singular value decomposition.

# Parameters
- `X::Matrix{Float64}`: The design matrix.
- `y::Vector{Float64}`: The response variable.
- `svd::Bool`: Whether to use the singular value decomposition.

# Returns
- `β::Vector{Float64}`: The estimated regression coefficients.
"""
function ordinary_least_squares(
  X::Matrix{Float64},
  y::Vector{Float64},
  svd_solver::Bool=true,
)::Vector{Float64}

  # Check that x and y have the same number of rows
  if size(X, 1) != length(y)
    throw(DimensionMismatch("X and y must have the same number of rows"))
  end

  if svd_solver
    # Compute coefficients using singular value decomposition
    U, s, V = svd(X)
    β = V * (s .\ (U' * y))
  else
    # Compute coefficients using the normal equation 
    β = X \ y # backslash operator uses QR decomposition by default
  end

  return β
end
```

### Ridge Regression
```{code} julia
:label: code-2
:caption: Ridge regression in Julia.

"""
Ridge regression

Implements the Ridge regression algorithm using the normal equation or the
singular value decomposition.

# Parameters
- `X::Matrix{Float64}`: The design matrix.
- `y::Vector{Float64}`: The response variable.
- `λ::Float64`: The regularization parameter.
- `svd::Bool`: Whether to use the singular value decomposition.

# Returns
- `β::Vector{Float64}`: The estimated coefficients.
"""
function ridge_regression(
  X::Matrix{Float64},
  y::Vector{Float64},
  λ::Float64,
  svd_solver::Bool=true,
)#::Vector{Float64}

  # Check that x and y have the same number of rows
  if size(X, 1) != length(y)
    throw(DimensionMismatch("X and y must have the same number of rows"))
  end

  if svd_solver
    U, s, V = svd(X)
    d = s ./ (s .^ 2 .+ λ)
    β = V * (d .* (U' * y))
  else
    β = inv(X' * X + λ * I) * X' * y
  end

  return β
end
```

### Lasso Regression
```{code} julia
:label: code-3
:caption: Lasso regression in Julia. Created with aid from [Claude 3.5 Sonnet](https://claude.ai/).

"""
LASSO regression

Implements the LASSO (least absolute shrinkage and selection operator) regression 
algorithm using the coordinate descent algorithm.

# Parameters
- `X::Matrix{Float64}`: The design matrix.
- `y::Vector{Float64}`: The response variable.
- `λ::Float64`: The regularization parameter.
- `max_iter::Int=1000`: The maximum number of iterations. Defaults to 1000.
- `tol::Float64=1e-6`: The tolerance for convergence. Defaults to 1e-6.

# Returns
- `β::Vector{Float64}`: The estimated coefficients.
"""
function lasso_regression(
  X::Matrix{Float64},
  y::Vector{Float64},
  λ::Float64,
  max_iter::Int=1000,
  tol::Float64=1e-6,
)::Vector{Float64}
  n, m = size(X) # n: number of samples, m: number of features

  # Check that x and y have the same number of rows
  if n != length(y)
    throw(DimensionMismatch("X and y must have the same number of rows"))
  end

  β = zeros(m)

  for _ = 1:max_iter
    β_old = copy(β)

    for j = 1:m
      # Calculate the residual without the j-th feature
      residual = y - X * β + X[:, j] * β[j]
      # Update the j-th coefficient using soft thresholding
      β[j] = soft_threshold(X[:, j]' * residual, λ) / (X[:, j]' * X[:, j])
    end

    # Check for convergence
    if norm(β - β_old) < tol
      break
    end
  end

  return β
end
```

```{code} julia
:label: code-4
:caption: Soft thresholding operator in Julia.
"""
Implements the soft thresholding operator.

# Parameters
- `x::Float64`: The value to be thresholded.
- `λ::Float64`: The regularization parameter.

# Returns
- `x_thresholded::Float64`: The thresholded value.
"""
function soft_threshold(x::Float64, λ::Float64)::Float64
  return sign(x) * max(abs(x) - λ, 0)
end
```

## 5th Degree Polynomial Regression Coefficients

:::{table} Regression coefficients ($β$) and their corresponding monomials for a polynomial of degree five. Each coefficient represents the weight assigned to the respective monomial term in the polynomial regression model, capturing the influence of different variable interactions on the response variable.
:label: table-3
:align: center

| Coefficient | Monomial |
| :-: | :-: |
| $\beta_0$ | $1$ |
| $\beta_1$ | $x_2^1$ |
| $\beta_2$ | $x_1^1$ |
| $\beta_3$ | $x_2^2$ |
| $\beta_4$ | $x_1^1 x_2^1$ |
| $\beta_5$ | $x_1^2$ |
| $\beta_6$ | $x_2^3$ |
| $\beta_7$ | $x_1^1 x_2^2$ |
| $\beta_8$ | $x_1^2 x_2^1$ |
| $\beta_9$ | $x_1^3$ |
| $\beta_{10}$ | $x_2^4$ |
| $\beta_{11}$ | $x_1^1 x_2^3$ |
| $\beta_{12}$ | $x_1^2 x_2^2$ |
| $\beta_{13}$ | $x_1^3 x_2^1$ |
| $\beta_{14}$ | $x_1^4$ |
| $\beta_{15}$ | $x_2^5$ |
| $\beta_{16}$ | $x_1^1 x_2^4$ |
| $\beta_{17}$ | $x_1^2 x_2^3$ |
| $\beta_{18}$ | $x_1^3 x_2^2$ |
| $\beta_{19}$ | $x_1^4 x_2^1$ |
| $\beta_{20}$ | $x_1^5$ |
:::